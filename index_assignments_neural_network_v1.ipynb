{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Layer Creation: Here we create the neural network with one hidden layer.\n",
    "\n",
    "# We create the neural network as the index assignment function from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11. 13.  8. 15. 10.  3.  9.  2.  6.  1.  0.  7. 14.  4.  5. 12.]\n",
      "0.6469959970475995\n"
     ]
    }
   ],
   "source": [
    "# 79-character line limit\n",
    "######### ######### ######### ######### ######### ######### ######### #########\n",
    "%reset -f\n",
    "\n",
    "import numpy as np # The NumPy library\n",
    "#import math # The math module, np includes it\n",
    "from scipy.integrate import quad # Method for integration in scipy.integrate sub-package\n",
    "\n",
    "\n",
    "cells_num = 16\n",
    "\n",
    "# The opitimized spaces for a Guassian distribution in a uniform quantizer(UQ).\n",
    "# The sizes are for UQs for number of cells: 4, 8, 16, 32, 64, 128, 256.\n",
    "all_size_gaus = np.array([0.9957, 0.5860, 0.3352, 0.1881, 0.1041, 0.0569, 0.0308])\n",
    "\n",
    "cell_size = all_size_gaus[int(np.log2(cells_num))-2]\n",
    "\n",
    "# np.arange has rounding error issue.\n",
    "# boundaries_symm = np.arange(-(cells_num/2-1) * cell_size, # For symmetric distributions\n",
    "#                              (cells_num/2) * cell_size,\n",
    "#                             cell_size) \n",
    "\n",
    "boundaries_symm = np.linspace(-(cells_num/2-1) * cell_size, # For symmetric distributions\n",
    "                               (cells_num/2-1) * cell_size,\n",
    "                              cells_num-1)\n",
    "n_inf = float(\"-inf\")\n",
    "p_inf = float(\"inf\")\n",
    "\n",
    "boundaries_symm = np.insert(boundaries_symm, 0, n_inf)\n",
    "boundaries_symm = np.append(boundaries_symm, p_inf)\n",
    "\n",
    "def pdf(x): # Defining the distribution\n",
    "    gaus_std = 1\n",
    "    gaus_mean = 0\n",
    "    pdf = 1/(gaus_std*np.sqrt(2*np.pi)) * \\\n",
    "                  np.exp(-0.5*((x-gaus_mean)/gaus_std)**2) # Gaussian pdf\n",
    "    return pdf\n",
    "\n",
    "def xpdf(x):\n",
    "    xpdf = x * pdf(x)\n",
    "    return xpdf\n",
    "\n",
    "def x2pdf (x):\n",
    "    x2pdf = x * xpdf(x)\n",
    "    return x2pdf\n",
    "\n",
    "prbs = []\n",
    "xprbs = []\n",
    "x2prbs = []\n",
    "cell_reps = []\n",
    "for i in range (0, cells_num):\n",
    "    cell_prb, integ_err = quad(pdf, boundaries_symm[i], boundaries_symm[i+1])\n",
    "    prbs = np.append(prbs, cell_prb)\n",
    "    \n",
    "    cell_xprb, integ_err = quad(xpdf, boundaries_symm[i], boundaries_symm[i+1])\n",
    "    xprbs = np.append(xprbs, cell_xprb)\n",
    "    \n",
    "    cell_x2prb, integ_err = quad(x2pdf, boundaries_symm[i], boundaries_symm[i+1])\n",
    "    x2prbs = np.append(x2prbs, cell_x2prb)\n",
    "    \n",
    "    cell_rep = cell_xprb / cell_prb\n",
    "    cell_reps = np.append(cell_reps, cell_rep)\n",
    "    \n",
    "  \n",
    "  \n",
    "\n",
    "######### ######### ######### ######### ######### ######### ######### #########\n",
    "l1_size = cells_num\n",
    "\n",
    "# # It is the weights and biases initialization.\n",
    "# l1_weights = (np.random.rand(l1_size, cells_num)-0.5)\n",
    "# # l1_biases = np.random.rand(l1_size,)-0.5\n",
    "# l1_biases = np.zeros(l1_size,)\n",
    "# l2_weights = (np.random.rand(cells_num, l1_size)-0.5)\n",
    "\n",
    "\n",
    "# It is the \"He Initialization\" technique.\n",
    "l1_weights = np.random.randn(l1_size, cells_num) * np.sqrt(2/cells_num)\n",
    "l1_biases = np.zeros(l1_size,)\n",
    "l2_weights = np.random.randn(cells_num, l1_size) * np.sqrt(2/l1_size)\n",
    "\n",
    "\n",
    "\n",
    "# We define the activation functions.\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def lrelu(x):\n",
    "    return np.where(x > 0, x, x * 0.1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x)) \n",
    "\n",
    "\n",
    "l1_outs = relu(np.dot(l1_weights, cell_reps)+l1_biases)\n",
    "l2_outs = np.dot(l2_weights, l1_outs)\n",
    "l2_outs = np.round(l2_outs, 16)\n",
    "\n",
    "# We assigned the indexes based on the final layer outputs.\n",
    "sorted_outs = np.sort(l2_outs)\n",
    "indexes = []\n",
    "for i in range(0, cells_num):\n",
    "    index = np.where(sorted_outs == l2_outs[i])\n",
    "    indexes = np.append(indexes, index)\n",
    "\n",
    "print(indexes)\n",
    "\n",
    "yijs = [] # Yijs based on the assigned indexes\n",
    "dijs = []\n",
    "for i in range(0, int(cells_num/2)):\n",
    "    j = i + cells_num/2\n",
    "    celli = np.where(indexes == i)\n",
    "    cellj = np.where(indexes == j)\n",
    "    \n",
    "    yij = (xprbs[celli] + xprbs[cellj])/(prbs[celli] + prbs[cellj])\n",
    "    yijs = np.append(yijs, yij)\n",
    "    \n",
    "    dij = x2prbs[celli] + yij**2 * prbs[celli] - yij *2 * xprbs[celli]\\\n",
    "        + x2prbs[cellj] + yij**2 * prbs[cellj] - yij *2 * xprbs[cellj]\n",
    "    dijs = np.append(dijs, dij)\n",
    "    \n",
    "distortion = np.round(sum(dijs), 16) # The Eve's distortion based on the assigned indexes\n",
    "\n",
    "print(distortion)\n",
    "\n",
    "past_velocity1 = 0 # Past velocity for momentum\n",
    "past_velocity_b1 = 0\n",
    "past_velocity2 = 0\n",
    "past_velocity_b2 = 0\n",
    "past_velocity3 = 0\n",
    "\n",
    "\n",
    "m_adam1 = 0 # Adam algorithm parameters\n",
    "v_adam1 = 0\n",
    "m_adam_b1 = 0\n",
    "v_adam_b1 = 0\n",
    "\n",
    "m_adam2 = 0\n",
    "v_adam2 = 0\n",
    "m_adam_b2 = 0\n",
    "v_adam_b2 = 0\n",
    "\n",
    "m_adam3 = 0\n",
    "v_adam3 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Layer Training: Here we train the neural network with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n"
     ]
    }
   ],
   "source": [
    "# This part provides the derivative of the distortion w.r.t. the weights\n",
    "# for the back propagation. The prefix 'b' shows that the variables are\n",
    "# calculated temporarily for the gradient descent\n",
    "\n",
    "learning_rate = 0.5\n",
    "epsilon = 0.5\n",
    " \n",
    "momentum = 0.1\n",
    "\n",
    "beta_1 = 0.999 # Parameters of Adam\n",
    "beta_2 = 0.9\n",
    "\n",
    "for ii in range (1, 100):\n",
    "    l1_gradients = [] # Gradients of layer 1 weights\n",
    "    # learning_rate = 0.1/ii\n",
    "    for i in range (0, l1_size):\n",
    "        for j in range (0, cells_num):\n",
    "            b_l1_weights = l1_weights.copy()\n",
    "            b_l1_weights[i][j] = l1_weights [i][j] + epsilon\n",
    "            b_l1_outs = relu(np.dot(b_l1_weights, cell_reps)+l1_biases)\n",
    "            b_l2_outs = np.dot(l2_weights, b_l1_outs)\n",
    "            b_l2_outs = np.round(b_l2_outs, 16)\n",
    "            \n",
    "            for p in range(0, cells_num-1):\n",
    "                for q in range (p+1, cells_num):\n",
    "                    if b_l2_outs[p] == b_l2_outs[q]:\n",
    "                        b_l2_outs[q] += np.random.rand(1,1)-0.5\n",
    "            \n",
    "            b_sorted_outs = np.sort(b_l2_outs)\n",
    "            b_indexes = []\n",
    "            for k in range(0, cells_num):\n",
    "                b_index = np.where(b_sorted_outs == b_l2_outs[k])\n",
    "                b_indexes = np.append(b_indexes, b_index)\n",
    "\n",
    "            b_yijs = [] \n",
    "            b_dijs = []\n",
    "            for m in range(0, int(cells_num/2)):\n",
    "                n = m + cells_num/2\n",
    "                b_celli = np.where(b_indexes == m)\n",
    "                b_cellj = np.where(b_indexes == n)\n",
    "\n",
    "                b_yij = (xprbs[b_celli] + xprbs[b_cellj])/(prbs[b_celli] + prbs[b_cellj])\n",
    "                b_yijs = np.append(b_yijs, b_yij)\n",
    "\n",
    "                b_dij = x2prbs[b_celli] + b_yij**2 * prbs[b_celli] - b_yij *2 * xprbs[b_celli]\\\n",
    "                      + x2prbs[b_cellj] + b_yij**2 * prbs[b_cellj] - b_yij *2 * xprbs[b_cellj]\n",
    "                b_dijs = np.append(b_dijs, b_dij)\n",
    "                        \n",
    "            b_distortion = np.round(sum(b_dijs), 16) \n",
    "            l1_gradient = (b_distortion - distortion)/epsilon\n",
    "            l1_gradients = np.append(l1_gradients, l1_gradient)\n",
    "            \n",
    "    l1_gradients = np.reshape(l1_gradients, [l1_size, cells_num])\n",
    "        \n",
    "        \n",
    "    l1_biases_gradients = [] # Gradients of the layer 1 biases\n",
    "    for i in range (0, l1_size):\n",
    "        b_l1_biases = l1_biases.copy()\n",
    "        b_l1_biases [i] = l1_biases [i] + epsilon\n",
    "        b_l1_outs = relu(np.dot(l1_weights, cell_reps)+b_l1_biases)\n",
    "        b_l2_outs = np.dot(l2_weights, b_l1_outs)\n",
    "        b_l2_outs = np.round(b_l2_outs, 16)\n",
    "        \n",
    "        for p in range(0, cells_num-1):\n",
    "            for q in range (p+1, cells_num):\n",
    "                if b_l2_outs[p] == b_l2_outs[q]:\n",
    "                    b_l2_outs[q] += np.random.rand(1,1)-0.5\n",
    "\n",
    "        b_sorted_outs = np.sort(b_l2_outs)\n",
    "        b_indexes = []\n",
    "        for k in range(0, cells_num):\n",
    "            b_index = np.where(b_sorted_outs == b_l2_outs[k])\n",
    "            b_indexes = np.append(b_indexes, b_index)\n",
    "\n",
    "        b_yijs = [] \n",
    "        b_dijs = []\n",
    "        for m in range(0, int(cells_num/2)):\n",
    "            n = m + cells_num/2\n",
    "            b_celli = np.where(b_indexes == m)\n",
    "            b_cellj = np.where(b_indexes == n)\n",
    "\n",
    "            b_yij = (xprbs[b_celli] + xprbs[b_cellj])/(prbs[b_celli] + prbs[b_cellj])\n",
    "            b_yijs = np.append(b_yijs, b_yij)\n",
    "\n",
    "            b_dij = x2prbs[b_celli] + b_yij**2 * prbs[b_celli] - b_yij *2 * xprbs[b_celli]\\\n",
    "                      + x2prbs[b_cellj] + b_yij**2 * prbs[b_cellj] - b_yij *2 * xprbs[b_cellj]\n",
    "            b_dijs = np.append(b_dijs, b_dij)\n",
    "                        \n",
    "        b_distortion = np.round(sum(b_dijs), 16) \n",
    "        l1_biases_gradient = (b_distortion - distortion)/epsilon\n",
    "        l1_biases_gradients = np.append(l1_biases_gradients, l1_biases_gradient)\n",
    "    \n",
    "           \n",
    "    l2_gradients = [] # Gradients of layer 2 weights\n",
    "    for i in range (0, cells_num):\n",
    "        for j in range (0, l1_size):\n",
    "            b_l2_weights = l2_weights.copy()\n",
    "            b_l2_weights[i][j] = l2_weights [i][j] + epsilon\n",
    "            b_l1_outs = relu(np.dot(l1_weights, cell_reps)+l1_biases)\n",
    "            b_l2_outs = np.dot(b_l2_weights, b_l1_outs)\n",
    "            b_l2_outs = np.round(b_l2_outs, 16)\n",
    "            \n",
    "            for p in range(0, cells_num-1):\n",
    "                for q in range (p+1, cells_num):\n",
    "                    if b_l2_outs[p] == b_l2_outs[q]:\n",
    "                        b_l2_outs[q] += np.random.rand(1,1)-0.5\n",
    "\n",
    "\n",
    "            b_sorted_outs = np.sort(b_l2_outs)\n",
    "            b_indexes = []\n",
    "            for k in range(0, cells_num):\n",
    "                b_index = np.where(b_sorted_outs == b_l2_outs[k])\n",
    "                b_indexes = np.append(b_indexes, b_index)\n",
    "\n",
    "            b_yijs = [] \n",
    "            b_dijs = []\n",
    "            for m in range(0, int(cells_num/2)):\n",
    "                n = m + cells_num/2\n",
    "                b_celli = np.where(b_indexes == m)\n",
    "                b_cellj = np.where(b_indexes == n)\n",
    "\n",
    "                b_yij = (xprbs[b_celli] + xprbs[b_cellj])/(prbs[b_celli] + prbs[b_cellj])\n",
    "                b_yijs = np.append(b_yijs, b_yij)\n",
    "\n",
    "                b_dij = x2prbs[b_celli] + b_yij**2 * prbs[b_celli] - b_yij *2 * xprbs[b_celli]\\\n",
    "                      + x2prbs[b_cellj] + b_yij**2 * prbs[b_cellj] - b_yij *2 * xprbs[b_cellj]\n",
    "                b_dijs = np.append(b_dijs, b_dij)\n",
    "                        \n",
    "            b_distortion = np.round(sum(b_dijs), 16) \n",
    "            l2_gradient = (b_distortion - distortion)/epsilon\n",
    "            l2_gradients = np.append(l2_gradients, l2_gradient)\n",
    "            \n",
    "    l2_gradients = np.reshape(l2_gradients, [cells_num, l1_size])\n",
    "\n",
    "# Now, we update the weights.   \n",
    "    l1_weights = l1_weights + learning_rate * l1_gradients\n",
    "    l1_biases = l1_biases + learning_rate * l1_biases_gradients\n",
    "    l2_weights = l2_weights + learning_rate * l2_gradients\n",
    "    \n",
    "#     # We update the weights with momentum.\n",
    "#     velocity1 = past_velocity1 * momentum + learning_rate *  l1_gradients\n",
    "#     l1_weights = l1_weights - momentum * velocity1 + learning_rate * l1_gradients\n",
    "#     past_velocity1 = velocity1 \n",
    "    \n",
    "#     velocity_b1 = past_velocity_b1 * momentum + learning_rate *  l1_biases_gradients\n",
    "#     l1_biases = l1_biases - momentum * velocity_b1 + learning_rate * l1_biases_gradients\n",
    "#     past_velocity_b1 = velocity_b1\n",
    "    \n",
    "#     velocity2 = past_velocity2 * momentum + learning_rate *  l2_gradients\n",
    "#     l2_weights = l2_weights - momentum * velocity2 + learning_rate * l2_gradients\n",
    "#     past_velocity2 = velocity2\n",
    "    \n",
    "#     # momentum way 2\n",
    "#     velocity1 = learning_rate * l1_gradients + momentum * past_velocity1\n",
    "#     l1_weights = l1_weights + velocity1\n",
    "#     past_velocity1 = velocity1\n",
    "    \n",
    "#     velocity2 = learning_rate * l2_gradients + momentum * past_velocity2\n",
    "#     l2_weights = l2_weights + velocity2\n",
    "#     past_velocity2 = velocity2\n",
    "    \n",
    "#     velocity_b1 = learning_rate * l1_biases_gradients + momentum * past_velocity_b1\n",
    "#     l1_biases = l1_biases + velocity_b1\n",
    "#     past_velocity_b1 = velocity_b1\n",
    "\n",
    "# # Here, we implement the Adam algorithm.\n",
    "\n",
    "#     m_adam1 = beta_1 * m_adam1 + (1 - beta_1) * l1_gradients\n",
    "#     v_adam1 = beta_2 * v_adam1 + (1 - beta_2) * np.power(l1_gradients, 2)\n",
    "#     m_hat1 = m_adam1 / (1 - np.power(beta_1, ii))\n",
    "#     v_hat1 = v_adam1 / (1 - np.power(beta_2, ii))\n",
    "#     l1_weights = l1_weights + learning_rate * m_hat1 / (np.sqrt(v_hat1) + epsilon)\n",
    "    \n",
    "#     m_adam_b1 = beta_1 * m_adam_b1 + (1 - beta_1) * l1_biases_gradients\n",
    "#     v_adam_b1 = beta_2 * v_adam_b1 + (1 - beta_2) * np.power(l1_biases_gradients, 2)\n",
    "#     m_hat_b1 = m_adam_b1 / (1 - np.power(beta_1, ii))\n",
    "#     v_hat_b1 = v_adam_b1 / (1 - np.power(beta_2, ii))\n",
    "#     l1_biases = l1_biases + learning_rate * m_hat_b1 / (np.sqrt(v_hat_b1) + epsilon)\n",
    "    \n",
    "#     m_adam2 = beta_1 * m_adam2 + (1 - beta_1) * l2_gradients\n",
    "#     v_adam2 = beta_2 * v_adam2 + (1 - beta_2) * np.power(l2_gradients, 2)\n",
    "#     m_hat2 = m_adam2 / (1 - np.power(beta_1, ii))\n",
    "#     v_hat2 = v_adam2 / (1 - np.power(beta_2, ii))\n",
    "#     l2_weights = l2_weights + learning_rate * m_hat2 / (np.sqrt(v_hat2) + epsilon)\n",
    "      \n",
    "    \n",
    "    l1_outs = relu(np.dot(l1_weights,cell_reps)+l1_biases)\n",
    "    l2_outs = np.dot(l2_weights, l1_outs)\n",
    "    l2_outs = np.round(l2_outs, 16)\n",
    "    \n",
    "    for p in range(0, cells_num-1):\n",
    "        for q in range (p+1, cells_num):\n",
    "            if l2_outs[p] == l2_outs[q]:\n",
    "                l2_outs[q] += np.random.rand(1,1)-0.5\n",
    "   \n",
    "\n",
    "    # We assigned the indexes based on the final layer outputs.\n",
    "    sorted_outs = np.sort(l2_outs)\n",
    "    indexes = []\n",
    "    for i in range(0, cells_num):\n",
    "        index = np.where(sorted_outs == l2_outs[i])\n",
    "        indexes = np.append(indexes, index)\n",
    "\n",
    "    #print(indexes)\n",
    "    \n",
    "    yijs = [] # Yijs based on the assigned indexes\n",
    "    dijs = []\n",
    "    for i in range(0, int(cells_num/2)):\n",
    "        j = i + cells_num/2\n",
    "        celli = np.where(indexes == i)\n",
    "        cellj = np.where(indexes == j)\n",
    "\n",
    "        yij = (xprbs[celli] + xprbs[cellj])/(prbs[celli] + prbs[cellj])\n",
    "        yijs = np.append(yijs, yij)\n",
    "\n",
    "        dij = x2prbs[celli] + yij**2 * prbs[celli] - yij *2 * xprbs[celli]\\\n",
    "            + x2prbs[cellj] + yij**2 * prbs[cellj] - yij *2 * xprbs[cellj]\n",
    "        dijs = np.append(dijs, dij)\n",
    "\n",
    "    distortion = np.round(sum(dijs), 16) # The Eve's distortion based on the assigned indexes\n",
    "    print(distortion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Layer Creation: Here we create the neural network with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.  9.  8. 12. 13.  0. 14.  7. 10.  6.  4.  5. 15.  2.  3.  1.]\n",
      "0.8313416094946148\n"
     ]
    }
   ],
   "source": [
    "# 79-character line limit\n",
    "######### ######### ######### ######### ######### ######### ######### #########\n",
    "%reset -f\n",
    "\n",
    "import numpy as np # The NumPy library\n",
    "#import math # The math module, np includes it\n",
    "from scipy.integrate import quad # Method for integration in scipy.integrate sub-package\n",
    "\n",
    "\n",
    "cells_num = 16\n",
    "\n",
    "# The opitimized spaces for a Guassian distribution in a uniform quantizer(UQ).\n",
    "# The sizes are for UQs for number of cells: 4, 8, 16, 32, 64, 128, 256.\n",
    "all_size_gaus = np.array([0.9957, 0.5860, 0.3352, 0.1881, 0.1041, 0.0569, 0.0308])\n",
    "\n",
    "cell_size = all_size_gaus[int(np.log2(cells_num))-2]\n",
    "\n",
    "# np.arange has rounding error issue.\n",
    "# boundaries_symm = np.arange(-(cells_num/2-1) * cell_size, # For symmetric distributions\n",
    "#                              (cells_num/2) * cell_size,\n",
    "#                             cell_size) \n",
    "\n",
    "boundaries_symm = np.linspace(-(cells_num/2-1) * cell_size, # For symmetric distributions\n",
    "                               (cells_num/2-1) * cell_size,\n",
    "                              cells_num-1)\n",
    "n_inf = float(\"-inf\")\n",
    "p_inf = float(\"inf\")\n",
    "\n",
    "boundaries_symm = np.insert(boundaries_symm, 0, n_inf)\n",
    "boundaries_symm = np.append(boundaries_symm, p_inf)\n",
    "\n",
    "def pdf(x): # Defining the distribution\n",
    "    gaus_std = 1\n",
    "    gaus_mean = 0\n",
    "    pdf = 1/(gaus_std*np.sqrt(2*np.pi)) * \\\n",
    "                  np.exp(-0.5*((x-gaus_mean)/gaus_std)**2) # Gaussian pdf\n",
    "    return pdf\n",
    "\n",
    "def xpdf(x):\n",
    "    xpdf = x * pdf(x)\n",
    "    return xpdf\n",
    "\n",
    "def x2pdf (x):\n",
    "    x2pdf = x * xpdf(x)\n",
    "    return x2pdf\n",
    "\n",
    "prbs = []\n",
    "xprbs = []\n",
    "x2prbs = []\n",
    "cell_reps = []\n",
    "for i in range (0, cells_num):\n",
    "    cell_prb, integ_err = quad(pdf, boundaries_symm[i], boundaries_symm[i+1])\n",
    "    prbs = np.append(prbs, cell_prb)\n",
    "    \n",
    "    cell_xprb, integ_err = quad(xpdf, boundaries_symm[i], boundaries_symm[i+1])\n",
    "    xprbs = np.append(xprbs, cell_xprb)\n",
    "    \n",
    "    cell_x2prb, integ_err = quad(x2pdf, boundaries_symm[i], boundaries_symm[i+1])\n",
    "    x2prbs = np.append(x2prbs, cell_x2prb)\n",
    "    \n",
    "    cell_rep = cell_xprb / cell_prb\n",
    "    cell_reps = np.append(cell_reps, cell_rep)\n",
    "    \n",
    "\n",
    "######### ######### ######### ######### ######### ######### ######### #########\n",
    "l1_size = cells_num \n",
    "l2_size = cells_num\n",
    "\n",
    "# # It is the weights and biases initialization.\n",
    "# l1_weights = np.random.rand(l1_size, cells_num)-0.5\n",
    "# l1_biases = np.random.rand(l1_size,)-0.5\n",
    "# l2_weights = np.random.rand(l2_size, l1_size)-0.5\n",
    "# l2_biases = np.random.rand(l2_size,)-0.5\n",
    "# l3_weights = np.random.rand(cells_num, l2_size)-0.5\n",
    "\n",
    "# It is the \"He Initialization\" technique.\n",
    "l1_weights = np.random.randn(l1_size, cells_num) * np.sqrt(2/cells_num)\n",
    "l1_biases = np.zeros(l1_size,)\n",
    "l2_weights = np.random.randn(l2_size, l1_size) * np.sqrt(2/l1_size)\n",
    "l2_biases = np.zeros(l2_size,)\n",
    "l3_weights = np.random.randn(cells_num, l2_size) * np.sqrt(2/l2_size)\n",
    "\n",
    "# We define the activation functions.\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "l1_outs = relu(np.dot(l1_weights, cell_reps)+l1_biases)\n",
    "l2_outs = relu(np.dot(l2_weights, l1_outs)+l2_biases)\n",
    "l3_outs = np.dot(l3_weights, l2_outs)\n",
    "l3_outs = np.round(l3_outs, 16)\n",
    "\n",
    "# We assigned the indexes based on the final layer outputs.\n",
    "sorted_outs = np.sort(l3_outs)\n",
    "indexes = []\n",
    "for i in range(0, cells_num):\n",
    "    index = np.where(sorted_outs == l3_outs[i])\n",
    "    indexes = np.append(indexes, index)\n",
    "\n",
    "print(indexes)\n",
    "\n",
    "yijs = [] # Yijs based on the assigned indexes\n",
    "dijs = []\n",
    "for i in range(0, int(cells_num/2)):\n",
    "    j = i + cells_num/2\n",
    "    celli = np.where(indexes == i)\n",
    "    cellj = np.where(indexes == j)\n",
    "    \n",
    "    yij = (xprbs[celli] + xprbs[cellj])/(prbs[celli] + prbs[cellj])\n",
    "    yijs = np.append(yijs, yij)\n",
    "    \n",
    "    dij = x2prbs[celli] + yij**2 * prbs[celli] - yij *2 * xprbs[celli]\\\n",
    "        + x2prbs[cellj] + yij**2 * prbs[cellj] - yij *2 * xprbs[cellj]\n",
    "    dijs = np.append(dijs, dij)\n",
    "    \n",
    "distortion = np.round(sum(dijs), 16) # The Eve's distortion based on the assigned indexes\n",
    "\n",
    "print(distortion)\n",
    "\n",
    "past_velocity1 = 0 # Past velocity for momentum\n",
    "past_velocity_b1 = 0\n",
    "past_velocity2 = 0\n",
    "past_velocity_b2 = 0\n",
    "past_velocity3 = 0\n",
    "\n",
    "\n",
    "m_adam1 = 0\n",
    "v_adam1 = 0\n",
    "m_adam_b1 = 0\n",
    "v_adam_b1 = 0\n",
    "\n",
    "m_adam2 = 0\n",
    "v_adam2 = 0\n",
    "m_adam_b2 = 0\n",
    "v_adam_b2 = 0\n",
    "\n",
    "m_adam3 = 0\n",
    "v_adam3 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Layers Training: Here we train the neural network with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n"
     ]
    }
   ],
   "source": [
    "# This part provides the derivative of the distortion w.r.t. the weights\n",
    "# for the back propagation. The prefix 'b' shows that the variables are\n",
    "# calculated temporarily for gradient descent\n",
    "\n",
    "momentum = 0.9 # Momentum for momentum algorithm\n",
    "\n",
    "beta_1 = 0.9 # Parameters of Adam\n",
    "beta_2 = 0.999\n",
    "\n",
    "\n",
    "learning_rate = 0.5\n",
    "epsilon = 0.5\n",
    "\n",
    "iteration_num = 100\n",
    "for ii in range (1, iteration_num):\n",
    "    l1_gradients = [] # Gradients of layer 1 weights\n",
    "    for i in range (0, l1_size):\n",
    "        for j in range (0, cells_num):\n",
    "            b_l1_weights = l1_weights.copy()\n",
    "            b_l1_weights[i][j] = l1_weights [i][j] + epsilon\n",
    "            b_l1_outs = relu(np.dot(b_l1_weights, cell_reps)+l1_biases)\n",
    "            b_l2_outs = relu(np.dot(l2_weights, b_l1_outs)+l2_biases)\n",
    "            b_l3_outs = np.dot(l3_weights, b_l2_outs)\n",
    "            b_l3_outs = np.round(b_l3_outs, 16)\n",
    "            \n",
    "            for p in range(0, cells_num-1):\n",
    "                for q in range (p+1, cells_num):\n",
    "                    if b_l3_outs[p] == b_l3_outs[q]:\n",
    "                        b_l3_outs[q] += np.random.rand(1,1)-0.5\n",
    "            \n",
    "            b_sorted_outs = np.sort(b_l3_outs)\n",
    "            b_indexes = []\n",
    "            for k in range(0, cells_num):\n",
    "                b_index = np.where(b_sorted_outs == b_l3_outs[k])\n",
    "                b_indexes = np.append(b_indexes, b_index)\n",
    "\n",
    "            b_yijs = [] \n",
    "            b_dijs = []\n",
    "            for m in range(0, int(cells_num/2)):\n",
    "                n = m + cells_num/2\n",
    "                b_celli = np.where(b_indexes == m)\n",
    "                b_cellj = np.where(b_indexes == n)\n",
    "\n",
    "                b_yij = (xprbs[b_celli] + xprbs[b_cellj])/(prbs[b_celli] + prbs[b_cellj])\n",
    "                b_yijs = np.append(b_yijs, b_yij)\n",
    "\n",
    "                b_dij = x2prbs[b_celli] + b_yij**2 * prbs[b_celli] - b_yij *2 * xprbs[b_celli]\\\n",
    "                      + x2prbs[b_cellj] + b_yij**2 * prbs[b_cellj] - b_yij *2 * xprbs[b_cellj]\n",
    "                b_dijs = np.append(b_dijs, b_dij)\n",
    "                        \n",
    "            b_distortion = np.round(sum(b_dijs), 16) \n",
    "            l1_gradient = (b_distortion - distortion)/epsilon\n",
    "            l1_gradients = np.append(l1_gradients, l1_gradient)\n",
    "            \n",
    "    l1_gradients = np.reshape(l1_gradients, [cells_num, cells_num])\n",
    "        \n",
    "        \n",
    "    l1_biases_gradients = [] # Gradients of the layer 1 biases\n",
    "    for i in range (0, l1_size):\n",
    "        b_l1_biases = l1_biases.copy()\n",
    "        b_l1_biases [i] = l1_biases [i] + epsilon\n",
    "        b_l1_outs = relu(np.dot(l1_weights, cell_reps)+b_l1_biases)\n",
    "        b_l2_outs = relu(np.dot(l2_weights, b_l1_outs)+l2_biases)\n",
    "        b_l3_outs = np.dot(l3_weights, b_l2_outs)\n",
    "        b_l3_outs = np.round(b_l3_outs, 16)\n",
    "        \n",
    "        for p in range(0, cells_num-1):\n",
    "            for q in range (p+1, cells_num):\n",
    "                if b_l3_outs[p] == b_l3_outs[q]:\n",
    "                    b_l3_outs[q] += np.random.rand(1,1)-0.5\n",
    "\n",
    "        b_sorted_outs = np.sort(b_l3_outs)\n",
    "        b_indexes = []\n",
    "        for k in range(0, cells_num):\n",
    "            b_index = np.where(b_sorted_outs == b_l3_outs[k])\n",
    "            b_indexes = np.append(b_indexes, b_index)\n",
    "\n",
    "        b_yijs = [] \n",
    "        b_dijs = []\n",
    "        for m in range(0, int(cells_num/2)):\n",
    "            n = m + cells_num/2\n",
    "            b_celli = np.where(b_indexes == m)\n",
    "            b_cellj = np.where(b_indexes == n)\n",
    "\n",
    "            b_yij = (xprbs[b_celli] + xprbs[b_cellj])/(prbs[b_celli] + prbs[b_cellj])\n",
    "            b_yijs = np.append(b_yijs, b_yij)\n",
    "\n",
    "            b_dij = x2prbs[b_celli] + b_yij**2 * prbs[b_celli] - b_yij *2 * xprbs[b_celli]\\\n",
    "                      + x2prbs[b_cellj] + b_yij**2 * prbs[b_cellj] - b_yij *2 * xprbs[b_cellj]\n",
    "            b_dijs = np.append(b_dijs, b_dij)\n",
    "                        \n",
    "        b_distortion = np.round(sum(b_dijs), 16) \n",
    "        l1_biases_gradient = (b_distortion - distortion)/epsilon\n",
    "        l1_biases_gradients = np.append(l1_biases_gradients, l1_biases_gradient)\n",
    "    \n",
    "           \n",
    "    l2_gradients = [] # Gradients of layer 2 weights\n",
    "    for i in range (0, l2_size):\n",
    "        for j in range (0, l1_size):\n",
    "            b_l2_weights = l2_weights.copy()\n",
    "            b_l2_weights[i][j] = l2_weights [i][j] + epsilon\n",
    "            b_l1_outs = relu(np.dot(l1_weights, cell_reps)+l1_biases)\n",
    "            b_l2_outs = relu(np.dot(b_l2_weights, b_l1_outs)+l2_biases)\n",
    "            b_l3_outs = np.dot(l3_weights, b_l2_outs)\n",
    "            b_l3_outs = np.round(b_l3_outs, 16)\n",
    "            \n",
    "            for p in range(0, cells_num):\n",
    "                for q in range (p+1, cells_num):\n",
    "                    if b_l3_outs[p] == b_l3_outs[q]:\n",
    "                        b_l3_outs[q] += np.random.rand(1,1)-0.5\n",
    "\n",
    "\n",
    "            b_sorted_outs = np.sort(b_l3_outs)\n",
    "            b_indexes = []\n",
    "            for k in range(0, cells_num):\n",
    "                b_index = np.where(b_sorted_outs == b_l3_outs[k])\n",
    "                b_indexes = np.append(b_indexes, b_index)\n",
    "\n",
    "            b_yijs = [] \n",
    "            b_dijs = []\n",
    "            for m in range(0, int(cells_num/2)):\n",
    "                n = m + cells_num/2\n",
    "                b_celli = np.where(b_indexes == m)\n",
    "                b_cellj = np.where(b_indexes == n)\n",
    "\n",
    "                b_yij = (xprbs[b_celli] + xprbs[b_cellj])/(prbs[b_celli] + prbs[b_cellj])\n",
    "                b_yijs = np.append(b_yijs, b_yij)\n",
    "\n",
    "                b_dij = x2prbs[b_celli] + b_yij**2 * prbs[b_celli] - b_yij *2 * xprbs[b_celli]\\\n",
    "                      + x2prbs[b_cellj] + b_yij**2 * prbs[b_cellj] - b_yij *2 * xprbs[b_cellj]\n",
    "                b_dijs = np.append(b_dijs, b_dij)\n",
    "                        \n",
    "            b_distortion = np.round(sum(b_dijs), 16) \n",
    "            l2_gradient = (b_distortion - distortion)/epsilon\n",
    "            l2_gradients = np.append(l2_gradients, l2_gradient)\n",
    "            \n",
    "    l2_gradients = np.reshape(l2_gradients, [cells_num, cells_num])\n",
    "    \n",
    "    \n",
    "    l2_biases_gradients = [] # Gradients of the layer 2 biases\n",
    "    for i in range (0, l2_size):\n",
    "        b_l2_biases = l2_biases.copy()\n",
    "        b_l2_biases [i] = l2_biases [i] + epsilon\n",
    "        b_l1_outs = relu(np.dot(l1_weights, cell_reps)+l1_biases)\n",
    "        b_l2_outs = relu(np.dot(l2_weights, b_l1_outs)+b_l2_biases)\n",
    "        b_l3_outs = np.dot(l3_weights, b_l2_outs)\n",
    "        b_l3_outs = np.round(b_l3_outs, 16)\n",
    "        \n",
    "        for p in range(0, cells_num-1):\n",
    "            for q in range (p+1, cells_num):\n",
    "                if b_l3_outs[p] == b_l3_outs[q]:\n",
    "                    b_l3_outs[q] += np.random.rand(1,1)-0.5\n",
    "\n",
    "        b_sorted_outs = np.sort(b_l3_outs)\n",
    "        b_indexes = []\n",
    "        for k in range(0, cells_num):\n",
    "            b_index = np.where(b_sorted_outs == b_l3_outs[k])\n",
    "            b_indexes = np.append(b_indexes, b_index)\n",
    "\n",
    "        b_yijs = [] \n",
    "        b_dijs = []\n",
    "        for m in range(0, int(cells_num/2)):\n",
    "            n = m + cells_num/2\n",
    "            b_celli = np.where(b_indexes == m)\n",
    "            b_cellj = np.where(b_indexes == n)\n",
    "\n",
    "            b_yij = (xprbs[b_celli] + xprbs[b_cellj])/(prbs[b_celli] + prbs[b_cellj])\n",
    "            b_yijs = np.append(b_yijs, b_yij)\n",
    "\n",
    "            b_dij = x2prbs[b_celli] + b_yij**2 * prbs[b_celli] - b_yij *2 * xprbs[b_celli]\\\n",
    "                      + x2prbs[b_cellj] + b_yij**2 * prbs[b_cellj] - b_yij *2 * xprbs[b_cellj]\n",
    "            b_dijs = np.append(b_dijs, b_dij)\n",
    "                        \n",
    "        b_distortion = np.round(sum(b_dijs), 16) \n",
    "        l2_biases_gradient = (b_distortion - distortion)/epsilon\n",
    "        l2_biases_gradients = np.append(l2_biases_gradients, l2_biases_gradient)\n",
    "        \n",
    "        \n",
    "    l3_gradients = [] # Gradients of layer 3 weights\n",
    "    for i in range (0, cells_num):\n",
    "        for j in range (0, l2_size):\n",
    "            b_l3_weights = l3_weights.copy()\n",
    "            b_l3_weights[i][j] = l3_weights [i][j] + epsilon\n",
    "            b_l1_outs = relu(np.dot(l1_weights, cell_reps)+l1_biases)\n",
    "            b_l2_outs = relu(np.dot(l2_weights, b_l1_outs)+l2_biases)\n",
    "            b_l3_outs = np.dot(b_l3_weights, b_l2_outs)\n",
    "            b_l3_outs = np.round(b_l3_outs, 16)\n",
    "            \n",
    "            for p in range(0, cells_num):\n",
    "                for q in range (p+1, cells_num):\n",
    "                    if b_l3_outs[p] == b_l3_outs[q]:\n",
    "                        b_l3_outs[q] += np.random.rand(1,1)-0.5\n",
    "\n",
    "\n",
    "            b_sorted_outs = np.sort(b_l3_outs)\n",
    "            b_indexes = []\n",
    "            for k in range(0, cells_num):\n",
    "                b_index = np.where(b_sorted_outs == b_l3_outs[k])\n",
    "                b_indexes = np.append(b_indexes, b_index)\n",
    "\n",
    "            b_yijs = [] \n",
    "            b_dijs = []\n",
    "            for m in range(0, int(cells_num/2)):\n",
    "                n = m + cells_num/2\n",
    "                b_celli = np.where(b_indexes == m)\n",
    "                b_cellj = np.where(b_indexes == n)\n",
    "\n",
    "                b_yij = (xprbs[b_celli] + xprbs[b_cellj])/(prbs[b_celli] + prbs[b_cellj])\n",
    "                b_yijs = np.append(b_yijs, b_yij)\n",
    "\n",
    "                b_dij = x2prbs[b_celli] + b_yij**2 * prbs[b_celli] - b_yij *2 * xprbs[b_celli]\\\n",
    "                      + x2prbs[b_cellj] + b_yij**2 * prbs[b_cellj] - b_yij *2 * xprbs[b_cellj]\n",
    "                b_dijs = np.append(b_dijs, b_dij)\n",
    "                        \n",
    "            b_distortion = np.round(sum(b_dijs), 16) \n",
    "            l3_gradient = (b_distortion - distortion)/epsilon\n",
    "            l3_gradients = np.append(l3_gradients, l3_gradient)\n",
    "            \n",
    "    l3_gradients = np.reshape(l3_gradients, [cells_num, cells_num])\n",
    "        \n",
    "#     # Now, we update the weights.   \n",
    "#     l1_weights = l1_weights + learning_rate * l1_gradients\n",
    "#     l1_biases = l1_biases + learning_rate * l1_biases_gradients\n",
    "#     l2_weights = l2_weights + learning_rate * l2_gradients    \n",
    "#     l2_biases = l2_biases + learning_rate * l2_biases_gradients\n",
    "#     l3_weights = l3_weights + learning_rate * l3_gradients\n",
    "    \n",
    "#     # We update the weights with momentum.\n",
    "#     velocity1 = past_velocity1 * momentum + learning_rate *  l1_gradients\n",
    "#     l1_weights = l1_weights - momentum * velocity1 + learning_rate * l1_gradients\n",
    "#     past_velocity1 = velocity1 \n",
    "    \n",
    "#     velocity_b1 = past_velocity_b1 * momentum + learning_rate *  l1_biases_gradients\n",
    "#     l1_biases = l1_biases - momentum * velocity_b1 + learning_rate * l1_biases_gradients\n",
    "#     past_velocity_b1 = velocity_b1\n",
    "    \n",
    "#     velocity2 = past_velocity2 * momentum + learning_rate *  l2_gradients\n",
    "#     l2_weights = l2_weights - momentum * velocity2 + learning_rate * l2_gradients\n",
    "#     past_velocity2 = velocity2\n",
    "    \n",
    "#     velocity_b2 = past_velocity_b2 * momentum + learning_rate *  l2_biases_gradients\n",
    "#     l2_biases = l2_biases - momentum * velocity_b2 + learning_rate * l2_biases_gradients\n",
    "#     past_velocity_b2 = velocity_b2\n",
    "    \n",
    "#     velocity3 = past_velocity3 * momentum + learning_rate *  l3_gradients\n",
    "#     l3_weights = l3_weights - momentum * velocity3 + learning_rate * l3_gradients\n",
    "#     past_velocity3 = velocity3\n",
    "\n",
    "\n",
    "\n",
    "    # momentum way 2\n",
    "    velocity1 = learning_rate * l1_gradients + momentum * past_velocity1\n",
    "    l1_weights = l1_weights + velocity1\n",
    "    past_velocity1 = velocity1\n",
    "    \n",
    "    velocity2 = learning_rate * l2_gradients + momentum * past_velocity2\n",
    "    l2_weights = l2_weights + velocity2\n",
    "    past_velocity2 = velocity2\n",
    "    \n",
    "    velocity_b1 = learning_rate * l1_biases_gradients + momentum * past_velocity_b1\n",
    "    l1_biases = l1_biases + velocity_b1\n",
    "    past_velocity_b1 = velocity_b1\n",
    "\n",
    "    velocity_b2 = learning_rate * l1_biases_gradients + momentum * past_velocity_b2\n",
    "    l2_biases = l2_biases + velocity_b2\n",
    "    past_velocity_b2 = velocity_b2\n",
    "\n",
    "    velocity3 = learning_rate * l3_gradients + momentum * past_velocity3\n",
    "    l3_weights = l3_weights + velocity3\n",
    "    past_velocity3 = velocity3\n",
    "\n",
    "\n",
    "# # Here, we implement the Adam algorithm.\n",
    "\n",
    "   \n",
    "#     m_adam1 = beta_1 * m_adam1 + (1 - beta_1) * l1_gradients\n",
    "#     v_adam1 = beta_2 * v_adam1 + (1 - beta_2) * np.power(l1_gradients, 2)\n",
    "#     m_hat1 = m_adam1 / (1 - np.power(beta_1, ii))\n",
    "#     v_hat1 = v_adam1 / (1 - np.power(beta_2, ii))\n",
    "#     l1_weights = l1_weights + learning_rate * m_hat1 / (np.sqrt(v_hat1) + epsilon)\n",
    "    \n",
    "#     m_adam_b1 = beta_1 * m_adam_b1 + (1 - beta_1) * l1_biases_gradients\n",
    "#     v_adam_b1 = beta_2 * v_adam_b1 + (1 - beta_2) * np.power(l1_biases_gradients, 2)\n",
    "#     m_hat_b1 = m_adam_b1 / (1 - np.power(beta_1, ii))\n",
    "#     v_hat_b1 = v_adam_b1 / (1 - np.power(beta_2, ii))\n",
    "#     l1_biases = l1_biases + learning_rate * m_hat_b1 / (np.sqrt(v_hat_b1) + epsilon)\n",
    "    \n",
    "#     m_adam2 = beta_1 * m_adam2 + (1 - beta_1) * l2_gradients\n",
    "#     v_adam2 = beta_2 * v_adam2 + (1 - beta_2) * np.power(l2_gradients, 2)\n",
    "#     m_hat2 = m_adam2 / (1 - np.power(beta_1, ii))\n",
    "#     v_hat2 = v_adam2 / (1 - np.power(beta_2, ii))\n",
    "#     l2_weights = l2_weights + learning_rate * m_hat2 / (np.sqrt(v_hat2) + epsilon)\n",
    "    \n",
    "#     m_adam_b2 = beta_1 * m_adam_b2 + (1 - beta_1) * l2_biases_gradients\n",
    "#     v_adam_b2 = beta_2 * v_adam_b2 + (1 - beta_2) * np.power(l2_biases_gradients, 2)\n",
    "#     m_hat_b2 = m_adam_b2 / (1 - np.power(beta_1, ii))\n",
    "#     v_hat_b2 = v_adam_b2 / (1 - np.power(beta_2, ii))\n",
    "#     l2_biases = l2_biases + learning_rate * m_hat_b2 / (np.sqrt(v_hat_b2) + epsilon)\n",
    "    \n",
    "#     m_adam3 = beta_1 * m_adam3 + (1 - beta_1) * l3_gradients\n",
    "#     v_adam3 = beta_2 * v_adam3 + (1 - beta_2) * np.power(l3_gradients, 2)\n",
    "#     m_hat3 = m_adam3 / (1 - np.power(beta_1, ii))\n",
    "#     v_hat3 = v_adam3 / (1 - np.power(beta_2, ii))\n",
    "#     l3_weights = l3_weights + learning_rate * m_hat3 / (np.sqrt(v_hat3) + epsilon)\n",
    "    \n",
    "        \n",
    "# Now, we calculate the outputs.\n",
    "    l1_outs = relu(np.dot(l1_weights,cell_reps)+l1_biases)\n",
    "    l2_outs = relu(np.dot(l2_weights,l1_outs)+l2_biases)\n",
    "    l3_outs = np.dot(l3_weights, l2_outs)\n",
    "    l3_outs = np.round(l3_outs, 16)\n",
    "    \n",
    "    for p in range(0, cells_num-1):\n",
    "        for q in range (p+1, cells_num):\n",
    "            if l3_outs[p] == l3_outs[q]:\n",
    "                l3_outs[q] += np.random.rand(1,1)-0.5\n",
    "   \n",
    "\n",
    "    # We assigned the indexes based on the final layer outputs.\n",
    "    sorted_outs = np.sort(l3_outs)\n",
    "    indexes = []\n",
    "    for i in range(0, cells_num):\n",
    "        index = np.where(sorted_outs == l3_outs[i])\n",
    "        indexes = np.append(indexes, index)\n",
    "\n",
    "    #print(indexes)\n",
    "    \n",
    "    yijs = [] # Yijs based on the assigned indexes\n",
    "    dijs = []\n",
    "    for i in range(0, int(cells_num/2)):\n",
    "        j = i + cells_num/2\n",
    "        celli = np.where(indexes == i)\n",
    "        cellj = np.where(indexes == j)\n",
    "\n",
    "        yij = (xprbs[celli] + xprbs[cellj])/(prbs[celli] + prbs[cellj])\n",
    "        yijs = np.append(yijs, yij)\n",
    "\n",
    "        dij = x2prbs[celli] + yij**2 * prbs[celli] - yij *2 * xprbs[celli]\\\n",
    "            + x2prbs[cellj] + yij**2 * prbs[cellj] - yij *2 * xprbs[cellj]\n",
    "        dijs = np.append(dijs, dij)\n",
    "\n",
    "    distortion = np.round(sum(dijs), 16) # The Eve's distortion based on the assigned indexes\n",
    "    print(distortion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 79-character line limit\n",
    "######### ######### ######### ######### ######### ######### ######### #########\n",
    "%reset -f\n",
    "\n",
    "import numpy as np # The NumPy library\n",
    "#import math # The math module, np includes it\n",
    "from scipy.integrate import quad # Method for integration in scipy.integrate sub-package\n",
    "\n",
    "\n",
    "cells_num = 4\n",
    "\n",
    "# The opitimized spaces for a Guassian distribution in a uniform quantizer(UQ).\n",
    "# The sizes are for UQs for number of cells: 4, 8, 16, 32, 64, 128, 256.\n",
    "all_size_gaus = np.array([0.9957, 0.5860, 0.3352, 0.1881, 0.1041, 0.0569, 0.0308])\n",
    "\n",
    "cell_size = all_size_gaus[int(np.log2(cells_num))-2]\n",
    "\n",
    "# np.arange has rounding error issue.\n",
    "# boundaries_symm = np.arange(-(cells_num/2-1) * cell_size, # For symmetric distributions\n",
    "#                              (cells_num/2) * cell_size,\n",
    "#                             cell_size) \n",
    "\n",
    "boundaries_symm = np.linspace(-(cells_num/2-1) * cell_size, # For symmetric distributions\n",
    "                               (cells_num/2-1) * cell_size,\n",
    "                              cells_num-1)\n",
    "n_inf = float(\"-inf\")\n",
    "p_inf = float(\"inf\")\n",
    "\n",
    "boundaries_symm = np.insert(boundaries_symm, 0, n_inf)\n",
    "boundaries_symm = np.append(boundaries_symm, p_inf)\n",
    "\n",
    "def pdf(x): # Defining the distribution\n",
    "    gaus_std = 1\n",
    "    gaus_mean = 0\n",
    "    pdf = 1/(gaus_std*np.sqrt(2*np.pi)) * \\\n",
    "                  np.exp(-0.5*((x-gaus_mean)/gaus_std)**2) # Gaussian pdf\n",
    "    return pdf\n",
    "\n",
    "def xpdf(x):\n",
    "    xpdf = x * pdf(x)\n",
    "    return xpdf\n",
    "\n",
    "def x2pdf (x):\n",
    "    x2pdf = x * xpdf(x)\n",
    "    return x2pdf\n",
    "\n",
    "prbs = []\n",
    "xprbs = []\n",
    "x2prbs = []\n",
    "cell_reps = []\n",
    "for i in range (0, cells_num):\n",
    "    cell_prb, integ_err = quad(pdf, boundaries_symm[i], boundaries_symm[i+1])\n",
    "    prbs = np.append(prbs, cell_prb)\n",
    "    \n",
    "    cell_xprb, integ_err = quad(xpdf, boundaries_symm[i], boundaries_symm[i+1])\n",
    "    xprbs = np.append(xprbs, cell_xprb)\n",
    "    \n",
    "    cell_x2prb, integ_err = quad(x2pdf, boundaries_symm[i], boundaries_symm[i+1])\n",
    "    x2prbs = np.append(x2prbs, cell_x2prb)\n",
    "    \n",
    "    cell_rep = cell_xprb / cell_prb\n",
    "    cell_reps = np.append(cell_reps, cell_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Constants: We prepare the constants for distortion calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 8)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'asd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-288-f58f4e703fe2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0ml1_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1_outs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0masd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0ml2_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcells_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'asd' is not defined"
     ]
    }
   ],
   "source": [
    "# We use the Functional API option of Keras.\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "l1_size = cells_num\n",
    "\n",
    "input_tensor = Input(shape=(cells_num,))\n",
    "l1_outs = layers.Dense(l1_size, activation = 'relu')(input_tensor)\n",
    "print(l1_outs.shape)\n",
    "l2_outs = layers.Dense(cells_num, activation = 'sigmoid')(l1_outs)\n",
    "\n",
    "# def make_indexes(x):\n",
    "#     raw_indexes = K.dot(x, cells_num)\n",
    "#     indexes = K.round(raw_indexes)\n",
    "#     for i in (0, cells_num-1):\n",
    "#         for j in (i+1, cells_num):\n",
    "#             if K.equal(indexes[i], indexes[j]):\n",
    "                \n",
    "#     return \n",
    "\n",
    "sorted_outs = layers.Lambda (sort_outs)(l2_outs)\n",
    "#def distortion ()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "[0.62398884 0.56634292 0.77043103 0.63150229 0.39028457 0.9778991\n",
      " 0.57253317 0.83175598 0.62398884 0.64544045 0.98116076 0.99303273]\n",
      "[[1. 0. 6. 2. 5. 3. 7. 4.]\n",
      " [4. 1. 7. 0. 3. 5. 2. 6.]\n",
      " [5. 4. 7. 3. 2. 1. 6. 0.]\n",
      " [3. 4. 0. 1. 2. 5. 7. 6.]\n",
      " [1. 5. 0. 3. 7. 2. 6. 4.]\n",
      " [6. 1. 0. 3. 7. 2. 4. 5.]\n",
      " [0. 5. 3. 4. 1. 7. 2. 6.]\n",
      " [6. 7. 4. 2. 1. 0. 3. 5.]\n",
      " [0. 6. 2. 1. 7. 3. 4. 5.]\n",
      " [7. 6. 1. 2. 3. 0. 5. 4.]\n",
      " [3. 4. 2. 5. 7. 6. 1. 0.]\n",
      " [6. 0. 7. 1. 2. 4. 3. 5.]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "print(np.math.factorial(4))\n",
    "\n",
    "distortions = []\n",
    "indexes_set = []\n",
    "\n",
    "for ii in range(0, 12):\n",
    "    indexes = random.sample(range(8), 8)\n",
    "    indexes = np.array(indexes)\n",
    "    \n",
    "    yijs = [] # Yijs based on the assigned indexes\n",
    "    dijs = []\n",
    "    for i in range(0, int(cells_num/2)):\n",
    "        j = i + cells_num/2\n",
    "        celli = np.where(indexes == i)\n",
    "        cellj = np.where(indexes == j)\n",
    "\n",
    "        yij = (xprbs[celli] + xprbs[cellj])/(prbs[celli] + prbs[cellj])\n",
    "        yijs = np.append(yijs, yij)\n",
    "\n",
    "        dij = x2prbs[celli] + yij**2 * prbs[celli] - yij *2 * xprbs[celli]\\\n",
    "            + x2prbs[cellj] + yij**2 * prbs[cellj] - yij *2 * xprbs[cellj]\n",
    "        dijs = np.append(dijs, dij)\n",
    "\n",
    "    distortion = np.round(sum(dijs), 16)\n",
    "    distortions = np.append(distortions, distortion)\n",
    "    indexes_set = np.append(indexes_set, indexes)\n",
    "    indexes_set = np.reshape(\n",
    "        indexes_set,(int(np.size(indexes_set)/cells_num), cells_num)) \n",
    "    \n",
    "print(distortions)\n",
    "print(indexes_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1 into shape (64,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-7d3e5a25c04b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m encoded_indexes_set = np.reshape(\n\u001b[0;32m---> 13\u001b[0;31m     int(np.size(encoded_indexes_set)/(cells_num*cells_num)), cells_num * cells_num)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    299\u001b[0m            [5, 6]])\n\u001b[1;32m    300\u001b[0m     \"\"\"\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1 into shape (64,)"
     ]
    }
   ],
   "source": [
    "encoded = to_categorical(indexes_set[1])\n",
    "print(encoded)\n",
    "input = np.reshape(encoded,(cells_num * cells_num,))\n",
    "np.shape(indexes_set)[0]\n",
    "\n",
    "encoded_indexes_set = []\n",
    "for i in range (0, np.shape(indexes_set)[0]):\n",
    "    encoded_indexes = to_categorical(indexes_set[i])\n",
    "    encoded_indexes = np.reshape(encoded_indexes,(cells_num * cells_num,))\n",
    "    encoded_indexes_set = np.append(encoded_indexes_set, encoded_indexes)\n",
    "    \n",
    "encoded_indexes_set = np.reshape(\n",
    "    int(np.size(encoded_indexes_set)/(cells_num*cells_num)), cells_num * cells_num)\n",
    "\n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
