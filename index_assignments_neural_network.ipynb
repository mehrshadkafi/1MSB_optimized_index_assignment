{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Layer Creation: Here we create the neural network with one hidden layer.\n",
    "\n",
    "# We create the neural network as the index assignment function from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11. 13.  8. 15. 10.  3.  9.  2.  6.  1.  0.  7. 14.  4.  5. 12.]\n",
      "0.6469959970475995\n"
     ]
    }
   ],
   "source": [
    "# 79-character line limit\n",
    "######### ######### ######### ######### ######### ######### ######### #########\n",
    "%reset -f\n",
    "\n",
    "import numpy as np # The NumPy library\n",
    "#import math # The math module, np includes it\n",
    "from scipy.integrate import quad # Method for integration in scipy.integrate sub-package\n",
    "\n",
    "\n",
    "cells_num = 16\n",
    "\n",
    "# The opitimized spaces for a Guassian distribution in a uniform quantizer(UQ).\n",
    "# The sizes are for UQs for number of cells: 4, 8, 16, 32, 64, 128, 256.\n",
    "all_size_gaus = np.array([0.9957, 0.5860, 0.3352, 0.1881, 0.1041, 0.0569, 0.0308])\n",
    "\n",
    "cell_size = all_size_gaus[int(np.log2(cells_num))-2]\n",
    "\n",
    "# np.arange has rounding error issue.\n",
    "# boundaries_symm = np.arange(-(cells_num/2-1) * cell_size, # For symmetric distributions\n",
    "#                              (cells_num/2) * cell_size,\n",
    "#                             cell_size) \n",
    "\n",
    "boundaries_symm = np.linspace(-(cells_num/2-1) * cell_size, # For symmetric distributions\n",
    "                               (cells_num/2-1) * cell_size,\n",
    "                              cells_num-1)\n",
    "n_inf = float(\"-inf\")\n",
    "p_inf = float(\"inf\")\n",
    "\n",
    "boundaries_symm = np.insert(boundaries_symm, 0, n_inf)\n",
    "boundaries_symm = np.append(boundaries_symm, p_inf)\n",
    "\n",
    "def pdf(x): # Defining the distribution\n",
    "    gaus_std = 1\n",
    "    gaus_mean = 0\n",
    "    pdf = 1/(gaus_std*np.sqrt(2*np.pi)) * \\\n",
    "                  np.exp(-0.5*((x-gaus_mean)/gaus_std)**2) # Gaussian pdf\n",
    "    return pdf\n",
    "\n",
    "def xpdf(x):\n",
    "    xpdf = x * pdf(x)\n",
    "    return xpdf\n",
    "\n",
    "def x2pdf (x):\n",
    "    x2pdf = x * xpdf(x)\n",
    "    return x2pdf\n",
    "\n",
    "prbs = []\n",
    "xprbs = []\n",
    "x2prbs = []\n",
    "cell_reps = []\n",
    "for i in range (0, cells_num):\n",
    "    cell_prb, integ_err = quad(pdf, boundaries_symm[i], boundaries_symm[i+1])\n",
    "    prbs = np.append(prbs, cell_prb)\n",
    "    \n",
    "    cell_xprb, integ_err = quad(xpdf, boundaries_symm[i], boundaries_symm[i+1])\n",
    "    xprbs = np.append(xprbs, cell_xprb)\n",
    "    \n",
    "    cell_x2prb, integ_err = quad(x2pdf, boundaries_symm[i], boundaries_symm[i+1])\n",
    "    x2prbs = np.append(x2prbs, cell_x2prb)\n",
    "    \n",
    "    cell_rep = cell_xprb / cell_prb\n",
    "    cell_reps = np.append(cell_reps, cell_rep)\n",
    "    \n",
    "  \n",
    "  \n",
    "\n",
    "######### ######### ######### ######### ######### ######### ######### #########\n",
    "l1_size = cells_num\n",
    "\n",
    "# # It is the weights and biases initialization.\n",
    "# l1_weights = (np.random.rand(l1_size, cells_num)-0.5)\n",
    "# # l1_biases = np.random.rand(l1_size,)-0.5\n",
    "# l1_biases = np.zeros(l1_size,)\n",
    "# l2_weights = (np.random.rand(cells_num, l1_size)-0.5)\n",
    "\n",
    "\n",
    "# It is the \"He Initialization\" technique.\n",
    "l1_weights = np.random.randn(l1_size, cells_num) * np.sqrt(2/cells_num)\n",
    "l1_biases = np.zeros(l1_size,)\n",
    "l2_weights = np.random.randn(cells_num, l1_size) * np.sqrt(2/l1_size)\n",
    "\n",
    "\n",
    "\n",
    "# We define the activation functions.\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def lrelu(x):\n",
    "    return np.where(x > 0, x, x * 0.1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x)) \n",
    "\n",
    "\n",
    "l1_outs = relu(np.dot(l1_weights, cell_reps)+l1_biases)\n",
    "l2_outs = np.dot(l2_weights, l1_outs)\n",
    "l2_outs = np.round(l2_outs, 16)\n",
    "\n",
    "# We assigned the indexes based on the final layer outputs.\n",
    "sorted_outs = np.sort(l2_outs)\n",
    "indexes = []\n",
    "for i in range(0, cells_num):\n",
    "    index = np.where(sorted_outs == l2_outs[i])\n",
    "    indexes = np.append(indexes, index)\n",
    "\n",
    "print(indexes)\n",
    "\n",
    "yijs = [] # Yijs based on the assigned indexes\n",
    "dijs = []\n",
    "for i in range(0, int(cells_num/2)):\n",
    "    j = i + cells_num/2\n",
    "    celli = np.where(indexes == i)\n",
    "    cellj = np.where(indexes == j)\n",
    "    \n",
    "    yij = (xprbs[celli] + xprbs[cellj])/(prbs[celli] + prbs[cellj])\n",
    "    yijs = np.append(yijs, yij)\n",
    "    \n",
    "    dij = x2prbs[celli] + yij**2 * prbs[celli] - yij *2 * xprbs[celli]\\\n",
    "        + x2prbs[cellj] + yij**2 * prbs[cellj] - yij *2 * xprbs[cellj]\n",
    "    dijs = np.append(dijs, dij)\n",
    "    \n",
    "distortion = np.round(sum(dijs), 16) # The Eve's distortion based on the assigned indexes\n",
    "\n",
    "print(distortion)\n",
    "\n",
    "past_velocity1 = 0 # Past velocity for momentum\n",
    "past_velocity_b1 = 0\n",
    "past_velocity2 = 0\n",
    "past_velocity_b2 = 0\n",
    "past_velocity3 = 0\n",
    "\n",
    "\n",
    "m_adam1 = 0 # Adam algorithm parameters\n",
    "v_adam1 = 0\n",
    "m_adam_b1 = 0\n",
    "v_adam_b1 = 0\n",
    "\n",
    "m_adam2 = 0\n",
    "v_adam2 = 0\n",
    "m_adam_b2 = 0\n",
    "v_adam_b2 = 0\n",
    "\n",
    "m_adam3 = 0\n",
    "v_adam3 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Layer Training: Here we train the neural network with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9865424420062542\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n",
      "0.9875373046650608\n"
     ]
    }
   ],
   "source": [
    "# This part provides the derivative of the distortion w.r.t. the weights\n",
    "# for the back propagation. The prefix 'b' shows that the variables are\n",
    "# calculated temporarily for the gradient descent\n",
    "\n",
    "learning_rate = 0.5\n",
    "epsilon = 0.5\n",
    " \n",
    "momentum = 0.1\n",
    "\n",
    "beta_1 = 0.999 # Parameters of Adam\n",
    "beta_2 = 0.9\n",
    "\n",
    "for ii in range (1, 100):\n",
    "    l1_gradients = [] # Gradients of layer 1 weights\n",
    "    # learning_rate = 0.1/ii\n",
    "    for i in range (0, l1_size):\n",
    "        for j in range (0, cells_num):\n",
    "            b_l1_weights = l1_weights.copy()\n",
    "            b_l1_weights[i][j] = l1_weights [i][j] + epsilon\n",
    "            b_l1_outs = relu(np.dot(b_l1_weights, cell_reps)+l1_biases)\n",
    "            b_l2_outs = np.dot(l2_weights, b_l1_outs)\n",
    "            b_l2_outs = np.round(b_l2_outs, 16)\n",
    "            \n",
    "            for p in range(0, cells_num-1):\n",
    "                for q in range (p+1, cells_num):\n",
    "                    if b_l2_outs[p] == b_l2_outs[q]:\n",
    "                        b_l2_outs[q] += np.random.rand(1,1)-0.5\n",
    "            \n",
    "            b_sorted_outs = np.sort(b_l2_outs)\n",
    "            b_indexes = []\n",
    "            for k in range(0, cells_num):\n",
    "                b_index = np.where(b_sorted_outs == b_l2_outs[k])\n",
    "                b_indexes = np.append(b_indexes, b_index)\n",
    "\n",
    "            b_yijs = [] \n",
    "            b_dijs = []\n",
    "            for m in range(0, int(cells_num/2)):\n",
    "                n = m + cells_num/2\n",
    "                b_celli = np.where(b_indexes == m)\n",
    "                b_cellj = np.where(b_indexes == n)\n",
    "\n",
    "                b_yij = (xprbs[b_celli] + xprbs[b_cellj])/(prbs[b_celli] + prbs[b_cellj])\n",
    "                b_yijs = np.append(b_yijs, b_yij)\n",
    "\n",
    "                b_dij = x2prbs[b_celli] + b_yij**2 * prbs[b_celli] - b_yij *2 * xprbs[b_celli]\\\n",
    "                      + x2prbs[b_cellj] + b_yij**2 * prbs[b_cellj] - b_yij *2 * xprbs[b_cellj]\n",
    "                b_dijs = np.append(b_dijs, b_dij)\n",
    "                        \n",
    "            b_distortion = np.round(sum(b_dijs), 16) \n",
    "            l1_gradient = (b_distortion - distortion)/epsilon\n",
    "            l1_gradients = np.append(l1_gradients, l1_gradient)\n",
    "            \n",
    "    l1_gradients = np.reshape(l1_gradients, [l1_size, cells_num])\n",
    "        \n",
    "        \n",
    "    l1_biases_gradients = [] # Gradients of the layer 1 biases\n",
    "    for i in range (0, l1_size):\n",
    "        b_l1_biases = l1_biases.copy()\n",
    "        b_l1_biases [i] = l1_biases [i] + epsilon\n",
    "        b_l1_outs = relu(np.dot(l1_weights, cell_reps)+b_l1_biases)\n",
    "        b_l2_outs = np.dot(l2_weights, b_l1_outs)\n",
    "        b_l2_outs = np.round(b_l2_outs, 16)\n",
    "        \n",
    "        for p in range(0, cells_num-1):\n",
    "            for q in range (p+1, cells_num):\n",
    "                if b_l2_outs[p] == b_l2_outs[q]:\n",
    "                    b_l2_outs[q] += np.random.rand(1,1)-0.5\n",
    "\n",
    "        b_sorted_outs = np.sort(b_l2_outs)\n",
    "        b_indexes = []\n",
    "        for k in range(0, cells_num):\n",
    "            b_index = np.where(b_sorted_outs == b_l2_outs[k])\n",
    "            b_indexes = np.append(b_indexes, b_index)\n",
    "\n",
    "        b_yijs = [] \n",
    "        b_dijs = []\n",
    "        for m in range(0, int(cells_num/2)):\n",
    "            n = m + cells_num/2\n",
    "            b_celli = np.where(b_indexes == m)\n",
    "            b_cellj = np.where(b_indexes == n)\n",
    "\n",
    "            b_yij = (xprbs[b_celli] + xprbs[b_cellj])/(prbs[b_celli] + prbs[b_cellj])\n",
    "            b_yijs = np.append(b_yijs, b_yij)\n",
    "\n",
    "            b_dij = x2prbs[b_celli] + b_yij**2 * prbs[b_celli] - b_yij *2 * xprbs[b_celli]\\\n",
    "                      + x2prbs[b_cellj] + b_yij**2 * prbs[b_cellj] - b_yij *2 * xprbs[b_cellj]\n",
    "            b_dijs = np.append(b_dijs, b_dij)\n",
    "                        \n",
    "        b_distortion = np.round(sum(b_dijs), 16) \n",
    "        l1_biases_gradient = (b_distortion - distortion)/epsilon\n",
    "        l1_biases_gradients = np.append(l1_biases_gradients, l1_biases_gradient)\n",
    "    \n",
    "           \n",
    "    l2_gradients = [] # Gradients of layer 2 weights\n",
    "    for i in range (0, cells_num):\n",
    "        for j in range (0, l1_size):\n",
    "            b_l2_weights = l2_weights.copy()\n",
    "            b_l2_weights[i][j] = l2_weights [i][j] + epsilon\n",
    "            b_l1_outs = relu(np.dot(l1_weights, cell_reps)+l1_biases)\n",
    "            b_l2_outs = np.dot(b_l2_weights, b_l1_outs)\n",
    "            b_l2_outs = np.round(b_l2_outs, 16)\n",
    "            \n",
    "            for p in range(0, cells_num-1):\n",
    "                for q in range (p+1, cells_num):\n",
    "                    if b_l2_outs[p] == b_l2_outs[q]:\n",
    "                        b_l2_outs[q] += np.random.rand(1,1)-0.5\n",
    "\n",
    "\n",
    "            b_sorted_outs = np.sort(b_l2_outs)\n",
    "            b_indexes = []\n",
    "            for k in range(0, cells_num):\n",
    "                b_index = np.where(b_sorted_outs == b_l2_outs[k])\n",
    "                b_indexes = np.append(b_indexes, b_index)\n",
    "\n",
    "            b_yijs = [] \n",
    "            b_dijs = []\n",
    "            for m in range(0, int(cells_num/2)):\n",
    "                n = m + cells_num/2\n",
    "                b_celli = np.where(b_indexes == m)\n",
    "                b_cellj = np.where(b_indexes == n)\n",
    "\n",
    "                b_yij = (xprbs[b_celli] + xprbs[b_cellj])/(prbs[b_celli] + prbs[b_cellj])\n",
    "                b_yijs = np.append(b_yijs, b_yij)\n",
    "\n",
    "                b_dij = x2prbs[b_celli] + b_yij**2 * prbs[b_celli] - b_yij *2 * xprbs[b_celli]\\\n",
    "                      + x2prbs[b_cellj] + b_yij**2 * prbs[b_cellj] - b_yij *2 * xprbs[b_cellj]\n",
    "                b_dijs = np.append(b_dijs, b_dij)\n",
    "                        \n",
    "            b_distortion = np.round(sum(b_dijs), 16) \n",
    "            l2_gradient = (b_distortion - distortion)/epsilon\n",
    "            l2_gradients = np.append(l2_gradients, l2_gradient)\n",
    "            \n",
    "    l2_gradients = np.reshape(l2_gradients, [cells_num, l1_size])\n",
    "\n",
    "# Now, we update the weights.   \n",
    "    l1_weights = l1_weights + learning_rate * l1_gradients\n",
    "    l1_biases = l1_biases + learning_rate * l1_biases_gradients\n",
    "    l2_weights = l2_weights + learning_rate * l2_gradients\n",
    "    \n",
    "#     # We update the weights with momentum.\n",
    "#     velocity1 = past_velocity1 * momentum + learning_rate *  l1_gradients\n",
    "#     l1_weights = l1_weights - momentum * velocity1 + learning_rate * l1_gradients\n",
    "#     past_velocity1 = velocity1 \n",
    "    \n",
    "#     velocity_b1 = past_velocity_b1 * momentum + learning_rate *  l1_biases_gradients\n",
    "#     l1_biases = l1_biases - momentum * velocity_b1 + learning_rate * l1_biases_gradients\n",
    "#     past_velocity_b1 = velocity_b1\n",
    "    \n",
    "#     velocity2 = past_velocity2 * momentum + learning_rate *  l2_gradients\n",
    "#     l2_weights = l2_weights - momentum * velocity2 + learning_rate * l2_gradients\n",
    "#     past_velocity2 = velocity2\n",
    "    \n",
    "#     # momentum way 2\n",
    "#     velocity1 = learning_rate * l1_gradients + momentum * past_velocity1\n",
    "#     l1_weights = l1_weights + velocity1\n",
    "#     past_velocity1 = velocity1\n",
    "    \n",
    "#     velocity2 = learning_rate * l2_gradients + momentum * past_velocity2\n",
    "#     l2_weights = l2_weights + velocity2\n",
    "#     past_velocity2 = velocity2\n",
    "    \n",
    "#     velocity_b1 = learning_rate * l1_biases_gradients + momentum * past_velocity_b1\n",
    "#     l1_biases = l1_biases + velocity_b1\n",
    "#     past_velocity_b1 = velocity_b1\n",
    "\n",
    "# # Here, we implement the Adam algorithm.\n",
    "\n",
    "#     m_adam1 = beta_1 * m_adam1 + (1 - beta_1) * l1_gradients\n",
    "#     v_adam1 = beta_2 * v_adam1 + (1 - beta_2) * np.power(l1_gradients, 2)\n",
    "#     m_hat1 = m_adam1 / (1 - np.power(beta_1, ii))\n",
    "#     v_hat1 = v_adam1 / (1 - np.power(beta_2, ii))\n",
    "#     l1_weights = l1_weights + learning_rate * m_hat1 / (np.sqrt(v_hat1) + epsilon)\n",
    "    \n",
    "#     m_adam_b1 = beta_1 * m_adam_b1 + (1 - beta_1) * l1_biases_gradients\n",
    "#     v_adam_b1 = beta_2 * v_adam_b1 + (1 - beta_2) * np.power(l1_biases_gradients, 2)\n",
    "#     m_hat_b1 = m_adam_b1 / (1 - np.power(beta_1, ii))\n",
    "#     v_hat_b1 = v_adam_b1 / (1 - np.power(beta_2, ii))\n",
    "#     l1_biases = l1_biases + learning_rate * m_hat_b1 / (np.sqrt(v_hat_b1) + epsilon)\n",
    "    \n",
    "#     m_adam2 = beta_1 * m_adam2 + (1 - beta_1) * l2_gradients\n",
    "#     v_adam2 = beta_2 * v_adam2 + (1 - beta_2) * np.power(l2_gradients, 2)\n",
    "#     m_hat2 = m_adam2 / (1 - np.power(beta_1, ii))\n",
    "#     v_hat2 = v_adam2 / (1 - np.power(beta_2, ii))\n",
    "#     l2_weights = l2_weights + learning_rate * m_hat2 / (np.sqrt(v_hat2) + epsilon)\n",
    "      \n",
    "    \n",
    "    l1_outs = relu(np.dot(l1_weights,cell_reps)+l1_biases)\n",
    "    l2_outs = np.dot(l2_weights, l1_outs)\n",
    "    l2_outs = np.round(l2_outs, 16)\n",
    "    \n",
    "    for p in range(0, cells_num-1):\n",
    "        for q in range (p+1, cells_num):\n",
    "            if l2_outs[p] == l2_outs[q]:\n",
    "                l2_outs[q] += np.random.rand(1,1)-0.5\n",
    "   \n",
    "\n",
    "    # We assigned the indexes based on the final layer outputs.\n",
    "    sorted_outs = np.sort(l2_outs)\n",
    "    indexes = []\n",
    "    for i in range(0, cells_num):\n",
    "        index = np.where(sorted_outs == l2_outs[i])\n",
    "        indexes = np.append(indexes, index)\n",
    "\n",
    "    #print(indexes)\n",
    "    \n",
    "    yijs = [] # Yijs based on the assigned indexes\n",
    "    dijs = []\n",
    "    for i in range(0, int(cells_num/2)):\n",
    "        j = i + cells_num/2\n",
    "        celli = np.where(indexes == i)\n",
    "        cellj = np.where(indexes == j)\n",
    "\n",
    "        yij = (xprbs[celli] + xprbs[cellj])/(prbs[celli] + prbs[cellj])\n",
    "        yijs = np.append(yijs, yij)\n",
    "\n",
    "        dij = x2prbs[celli] + yij**2 * prbs[celli] - yij *2 * xprbs[celli]\\\n",
    "            + x2prbs[cellj] + yij**2 * prbs[cellj] - yij *2 * xprbs[cellj]\n",
    "        dijs = np.append(dijs, dij)\n",
    "\n",
    "    distortion = np.round(sum(dijs), 16) # The Eve's distortion based on the assigned indexes\n",
    "    print(distortion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Layer Creation: Here we create the neural network with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.  9.  8. 12. 13.  0. 14.  7. 10.  6.  4.  5. 15.  2.  3.  1.]\n",
      "0.8313416094946148\n"
     ]
    }
   ],
   "source": [
    "# 79-character line limit\n",
    "######### ######### ######### ######### ######### ######### ######### #########\n",
    "%reset -f\n",
    "\n",
    "import numpy as np # The NumPy library\n",
    "#import math # The math module, np includes it\n",
    "from scipy.integrate import quad # Method for integration in scipy.integrate sub-package\n",
    "\n",
    "\n",
    "cells_num = 16\n",
    "\n",
    "# The opitimized spaces for a Guassian distribution in a uniform quantizer(UQ).\n",
    "# The sizes are for UQs for number of cells: 4, 8, 16, 32, 64, 128, 256.\n",
    "all_size_gaus = np.array([0.9957, 0.5860, 0.3352, 0.1881, 0.1041, 0.0569, 0.0308])\n",
    "\n",
    "cell_size = all_size_gaus[int(np.log2(cells_num))-2]\n",
    "\n",
    "# np.arange has rounding error issue.\n",
    "# boundaries_symm = np.arange(-(cells_num/2-1) * cell_size, # For symmetric distributions\n",
    "#                              (cells_num/2) * cell_size,\n",
    "#                             cell_size) \n",
    "\n",
    "boundaries_symm = np.linspace(-(cells_num/2-1) * cell_size, # For symmetric distributions\n",
    "                               (cells_num/2-1) * cell_size,\n",
    "                              cells_num-1)\n",
    "n_inf = float(\"-inf\")\n",
    "p_inf = float(\"inf\")\n",
    "\n",
    "boundaries_symm = np.insert(boundaries_symm, 0, n_inf)\n",
    "boundaries_symm = np.append(boundaries_symm, p_inf)\n",
    "\n",
    "def pdf(x): # Defining the distribution\n",
    "    gaus_std = 1\n",
    "    gaus_mean = 0\n",
    "    pdf = 1/(gaus_std*np.sqrt(2*np.pi)) * \\\n",
    "                  np.exp(-0.5*((x-gaus_mean)/gaus_std)**2) # Gaussian pdf\n",
    "    return pdf\n",
    "\n",
    "def xpdf(x):\n",
    "    xpdf = x * pdf(x)\n",
    "    return xpdf\n",
    "\n",
    "def x2pdf (x):\n",
    "    x2pdf = x * xpdf(x)\n",
    "    return x2pdf\n",
    "\n",
    "prbs = []\n",
    "xprbs = []\n",
    "x2prbs = []\n",
    "cell_reps = []\n",
    "for i in range (0, cells_num):\n",
    "    cell_prb, integ_err = quad(pdf, boundaries_symm[i], boundaries_symm[i+1])\n",
    "    prbs = np.append(prbs, cell_prb)\n",
    "    \n",
    "    cell_xprb, integ_err = quad(xpdf, boundaries_symm[i], boundaries_symm[i+1])\n",
    "    xprbs = np.append(xprbs, cell_xprb)\n",
    "    \n",
    "    cell_x2prb, integ_err = quad(x2pdf, boundaries_symm[i], boundaries_symm[i+1])\n",
    "    x2prbs = np.append(x2prbs, cell_x2prb)\n",
    "    \n",
    "    cell_rep = cell_xprb / cell_prb\n",
    "    cell_reps = np.append(cell_reps, cell_rep)\n",
    "    \n",
    "\n",
    "######### ######### ######### ######### ######### ######### ######### #########\n",
    "l1_size = cells_num \n",
    "l2_size = cells_num\n",
    "\n",
    "# # It is the weights and biases initialization.\n",
    "# l1_weights = np.random.rand(l1_size, cells_num)-0.5\n",
    "# l1_biases = np.random.rand(l1_size,)-0.5\n",
    "# l2_weights = np.random.rand(l2_size, l1_size)-0.5\n",
    "# l2_biases = np.random.rand(l2_size,)-0.5\n",
    "# l3_weights = np.random.rand(cells_num, l2_size)-0.5\n",
    "\n",
    "# It is the \"He Initialization\" technique.\n",
    "l1_weights = np.random.randn(l1_size, cells_num) * np.sqrt(2/cells_num)\n",
    "l1_biases = np.zeros(l1_size,)\n",
    "l2_weights = np.random.randn(l2_size, l1_size) * np.sqrt(2/l1_size)\n",
    "l2_biases = np.zeros(l2_size,)\n",
    "l3_weights = np.random.randn(cells_num, l2_size) * np.sqrt(2/l2_size)\n",
    "\n",
    "# We define the activation functions.\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "l1_outs = relu(np.dot(l1_weights, cell_reps)+l1_biases)\n",
    "l2_outs = relu(np.dot(l2_weights, l1_outs)+l2_biases)\n",
    "l3_outs = np.dot(l3_weights, l2_outs)\n",
    "l3_outs = np.round(l3_outs, 16)\n",
    "\n",
    "# We assigned the indexes based on the final layer outputs.\n",
    "sorted_outs = np.sort(l3_outs)\n",
    "indexes = []\n",
    "for i in range(0, cells_num):\n",
    "    index = np.where(sorted_outs == l3_outs[i])\n",
    "    indexes = np.append(indexes, index)\n",
    "\n",
    "print(indexes)\n",
    "\n",
    "yijs = [] # Yijs based on the assigned indexes\n",
    "dijs = []\n",
    "for i in range(0, int(cells_num/2)):\n",
    "    j = i + cells_num/2\n",
    "    celli = np.where(indexes == i)\n",
    "    cellj = np.where(indexes == j)\n",
    "    \n",
    "    yij = (xprbs[celli] + xprbs[cellj])/(prbs[celli] + prbs[cellj])\n",
    "    yijs = np.append(yijs, yij)\n",
    "    \n",
    "    dij = x2prbs[celli] + yij**2 * prbs[celli] - yij *2 * xprbs[celli]\\\n",
    "        + x2prbs[cellj] + yij**2 * prbs[cellj] - yij *2 * xprbs[cellj]\n",
    "    dijs = np.append(dijs, dij)\n",
    "    \n",
    "distortion = np.round(sum(dijs), 16) # The Eve's distortion based on the assigned indexes\n",
    "\n",
    "print(distortion)\n",
    "\n",
    "past_velocity1 = 0 # Past velocity for momentum\n",
    "past_velocity_b1 = 0\n",
    "past_velocity2 = 0\n",
    "past_velocity_b2 = 0\n",
    "past_velocity3 = 0\n",
    "\n",
    "\n",
    "m_adam1 = 0\n",
    "v_adam1 = 0\n",
    "m_adam_b1 = 0\n",
    "v_adam_b1 = 0\n",
    "\n",
    "m_adam2 = 0\n",
    "v_adam2 = 0\n",
    "m_adam_b2 = 0\n",
    "v_adam_b2 = 0\n",
    "\n",
    "m_adam3 = 0\n",
    "v_adam3 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Layers Training: Here we train the neural network with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n",
      "0.9480901976685344\n"
     ]
    }
   ],
   "source": [
    "# This part provides the derivative of the distortion w.r.t. the weights\n",
    "# for the back propagation. The prefix 'b' shows that the variables are\n",
    "# calculated temporarily for gradient descent\n",
    "\n",
    "momentum = 0.9 # Momentum for momentum algorithm\n",
    "\n",
    "beta_1 = 0.9 # Parameters of Adam\n",
    "beta_2 = 0.999\n",
    "\n",
    "\n",
    "learning_rate = 0.5\n",
    "epsilon = 0.5\n",
    "\n",
    "iteration_num = 100\n",
    "for ii in range (1, iteration_num):\n",
    "    l1_gradients = [] # Gradients of layer 1 weights\n",
    "    for i in range (0, l1_size):\n",
    "        for j in range (0, cells_num):\n",
    "            b_l1_weights = l1_weights.copy()\n",
    "            b_l1_weights[i][j] = l1_weights [i][j] + epsilon\n",
    "            b_l1_outs = relu(np.dot(b_l1_weights, cell_reps)+l1_biases)\n",
    "            b_l2_outs = relu(np.dot(l2_weights, b_l1_outs)+l2_biases)\n",
    "            b_l3_outs = np.dot(l3_weights, b_l2_outs)\n",
    "            b_l3_outs = np.round(b_l3_outs, 16)\n",
    "            \n",
    "            for p in range(0, cells_num-1):\n",
    "                for q in range (p+1, cells_num):\n",
    "                    if b_l3_outs[p] == b_l3_outs[q]:\n",
    "                        b_l3_outs[q] += np.random.rand(1,1)-0.5\n",
    "            \n",
    "            b_sorted_outs = np.sort(b_l3_outs)\n",
    "            b_indexes = []\n",
    "            for k in range(0, cells_num):\n",
    "                b_index = np.where(b_sorted_outs == b_l3_outs[k])\n",
    "                b_indexes = np.append(b_indexes, b_index)\n",
    "\n",
    "            b_yijs = [] \n",
    "            b_dijs = []\n",
    "            for m in range(0, int(cells_num/2)):\n",
    "                n = m + cells_num/2\n",
    "                b_celli = np.where(b_indexes == m)\n",
    "                b_cellj = np.where(b_indexes == n)\n",
    "\n",
    "                b_yij = (xprbs[b_celli] + xprbs[b_cellj])/(prbs[b_celli] + prbs[b_cellj])\n",
    "                b_yijs = np.append(b_yijs, b_yij)\n",
    "\n",
    "                b_dij = x2prbs[b_celli] + b_yij**2 * prbs[b_celli] - b_yij *2 * xprbs[b_celli]\\\n",
    "                      + x2prbs[b_cellj] + b_yij**2 * prbs[b_cellj] - b_yij *2 * xprbs[b_cellj]\n",
    "                b_dijs = np.append(b_dijs, b_dij)\n",
    "                        \n",
    "            b_distortion = np.round(sum(b_dijs), 16) \n",
    "            l1_gradient = (b_distortion - distortion)/epsilon\n",
    "            l1_gradients = np.append(l1_gradients, l1_gradient)\n",
    "            \n",
    "    l1_gradients = np.reshape(l1_gradients, [cells_num, cells_num])\n",
    "        \n",
    "        \n",
    "    l1_biases_gradients = [] # Gradients of the layer 1 biases\n",
    "    for i in range (0, l1_size):\n",
    "        b_l1_biases = l1_biases.copy()\n",
    "        b_l1_biases [i] = l1_biases [i] + epsilon\n",
    "        b_l1_outs = relu(np.dot(l1_weights, cell_reps)+b_l1_biases)\n",
    "        b_l2_outs = relu(np.dot(l2_weights, b_l1_outs)+l2_biases)\n",
    "        b_l3_outs = np.dot(l3_weights, b_l2_outs)\n",
    "        b_l3_outs = np.round(b_l3_outs, 16)\n",
    "        \n",
    "        for p in range(0, cells_num-1):\n",
    "            for q in range (p+1, cells_num):\n",
    "                if b_l3_outs[p] == b_l3_outs[q]:\n",
    "                    b_l3_outs[q] += np.random.rand(1,1)-0.5\n",
    "\n",
    "        b_sorted_outs = np.sort(b_l3_outs)\n",
    "        b_indexes = []\n",
    "        for k in range(0, cells_num):\n",
    "            b_index = np.where(b_sorted_outs == b_l3_outs[k])\n",
    "            b_indexes = np.append(b_indexes, b_index)\n",
    "\n",
    "        b_yijs = [] \n",
    "        b_dijs = []\n",
    "        for m in range(0, int(cells_num/2)):\n",
    "            n = m + cells_num/2\n",
    "            b_celli = np.where(b_indexes == m)\n",
    "            b_cellj = np.where(b_indexes == n)\n",
    "\n",
    "            b_yij = (xprbs[b_celli] + xprbs[b_cellj])/(prbs[b_celli] + prbs[b_cellj])\n",
    "            b_yijs = np.append(b_yijs, b_yij)\n",
    "\n",
    "            b_dij = x2prbs[b_celli] + b_yij**2 * prbs[b_celli] - b_yij *2 * xprbs[b_celli]\\\n",
    "                      + x2prbs[b_cellj] + b_yij**2 * prbs[b_cellj] - b_yij *2 * xprbs[b_cellj]\n",
    "            b_dijs = np.append(b_dijs, b_dij)\n",
    "                        \n",
    "        b_distortion = np.round(sum(b_dijs), 16) \n",
    "        l1_biases_gradient = (b_distortion - distortion)/epsilon\n",
    "        l1_biases_gradients = np.append(l1_biases_gradients, l1_biases_gradient)\n",
    "    \n",
    "           \n",
    "    l2_gradients = [] # Gradients of layer 2 weights\n",
    "    for i in range (0, l2_size):\n",
    "        for j in range (0, l1_size):\n",
    "            b_l2_weights = l2_weights.copy()\n",
    "            b_l2_weights[i][j] = l2_weights [i][j] + epsilon\n",
    "            b_l1_outs = relu(np.dot(l1_weights, cell_reps)+l1_biases)\n",
    "            b_l2_outs = relu(np.dot(b_l2_weights, b_l1_outs)+l2_biases)\n",
    "            b_l3_outs = np.dot(l3_weights, b_l2_outs)\n",
    "            b_l3_outs = np.round(b_l3_outs, 16)\n",
    "            \n",
    "            for p in range(0, cells_num):\n",
    "                for q in range (p+1, cells_num):\n",
    "                    if b_l3_outs[p] == b_l3_outs[q]:\n",
    "                        b_l3_outs[q] += np.random.rand(1,1)-0.5\n",
    "\n",
    "\n",
    "            b_sorted_outs = np.sort(b_l3_outs)\n",
    "            b_indexes = []\n",
    "            for k in range(0, cells_num):\n",
    "                b_index = np.where(b_sorted_outs == b_l3_outs[k])\n",
    "                b_indexes = np.append(b_indexes, b_index)\n",
    "\n",
    "            b_yijs = [] \n",
    "            b_dijs = []\n",
    "            for m in range(0, int(cells_num/2)):\n",
    "                n = m + cells_num/2\n",
    "                b_celli = np.where(b_indexes == m)\n",
    "                b_cellj = np.where(b_indexes == n)\n",
    "\n",
    "                b_yij = (xprbs[b_celli] + xprbs[b_cellj])/(prbs[b_celli] + prbs[b_cellj])\n",
    "                b_yijs = np.append(b_yijs, b_yij)\n",
    "\n",
    "                b_dij = x2prbs[b_celli] + b_yij**2 * prbs[b_celli] - b_yij *2 * xprbs[b_celli]\\\n",
    "                      + x2prbs[b_cellj] + b_yij**2 * prbs[b_cellj] - b_yij *2 * xprbs[b_cellj]\n",
    "                b_dijs = np.append(b_dijs, b_dij)\n",
    "                        \n",
    "            b_distortion = np.round(sum(b_dijs), 16) \n",
    "            l2_gradient = (b_distortion - distortion)/epsilon\n",
    "            l2_gradients = np.append(l2_gradients, l2_gradient)\n",
    "            \n",
    "    l2_gradients = np.reshape(l2_gradients, [cells_num, cells_num])\n",
    "    \n",
    "    \n",
    "    l2_biases_gradients = [] # Gradients of the layer 2 biases\n",
    "    for i in range (0, l2_size):\n",
    "        b_l2_biases = l2_biases.copy()\n",
    "        b_l2_biases [i] = l2_biases [i] + epsilon\n",
    "        b_l1_outs = relu(np.dot(l1_weights, cell_reps)+l1_biases)\n",
    "        b_l2_outs = relu(np.dot(l2_weights, b_l1_outs)+b_l2_biases)\n",
    "        b_l3_outs = np.dot(l3_weights, b_l2_outs)\n",
    "        b_l3_outs = np.round(b_l3_outs, 16)\n",
    "        \n",
    "        for p in range(0, cells_num-1):\n",
    "            for q in range (p+1, cells_num):\n",
    "                if b_l3_outs[p] == b_l3_outs[q]:\n",
    "                    b_l3_outs[q] += np.random.rand(1,1)-0.5\n",
    "\n",
    "        b_sorted_outs = np.sort(b_l3_outs)\n",
    "        b_indexes = []\n",
    "        for k in range(0, cells_num):\n",
    "            b_index = np.where(b_sorted_outs == b_l3_outs[k])\n",
    "            b_indexes = np.append(b_indexes, b_index)\n",
    "\n",
    "        b_yijs = [] \n",
    "        b_dijs = []\n",
    "        for m in range(0, int(cells_num/2)):\n",
    "            n = m + cells_num/2\n",
    "            b_celli = np.where(b_indexes == m)\n",
    "            b_cellj = np.where(b_indexes == n)\n",
    "\n",
    "            b_yij = (xprbs[b_celli] + xprbs[b_cellj])/(prbs[b_celli] + prbs[b_cellj])\n",
    "            b_yijs = np.append(b_yijs, b_yij)\n",
    "\n",
    "            b_dij = x2prbs[b_celli] + b_yij**2 * prbs[b_celli] - b_yij *2 * xprbs[b_celli]\\\n",
    "                      + x2prbs[b_cellj] + b_yij**2 * prbs[b_cellj] - b_yij *2 * xprbs[b_cellj]\n",
    "            b_dijs = np.append(b_dijs, b_dij)\n",
    "                        \n",
    "        b_distortion = np.round(sum(b_dijs), 16) \n",
    "        l2_biases_gradient = (b_distortion - distortion)/epsilon\n",
    "        l2_biases_gradients = np.append(l2_biases_gradients, l2_biases_gradient)\n",
    "        \n",
    "        \n",
    "    l3_gradients = [] # Gradients of layer 3 weights\n",
    "    for i in range (0, cells_num):\n",
    "        for j in range (0, l2_size):\n",
    "            b_l3_weights = l3_weights.copy()\n",
    "            b_l3_weights[i][j] = l3_weights [i][j] + epsilon\n",
    "            b_l1_outs = relu(np.dot(l1_weights, cell_reps)+l1_biases)\n",
    "            b_l2_outs = relu(np.dot(l2_weights, b_l1_outs)+l2_biases)\n",
    "            b_l3_outs = np.dot(b_l3_weights, b_l2_outs)\n",
    "            b_l3_outs = np.round(b_l3_outs, 16)\n",
    "            \n",
    "            for p in range(0, cells_num):\n",
    "                for q in range (p+1, cells_num):\n",
    "                    if b_l3_outs[p] == b_l3_outs[q]:\n",
    "                        b_l3_outs[q] += np.random.rand(1,1)-0.5\n",
    "\n",
    "\n",
    "            b_sorted_outs = np.sort(b_l3_outs)\n",
    "            b_indexes = []\n",
    "            for k in range(0, cells_num):\n",
    "                b_index = np.where(b_sorted_outs == b_l3_outs[k])\n",
    "                b_indexes = np.append(b_indexes, b_index)\n",
    "\n",
    "            b_yijs = [] \n",
    "            b_dijs = []\n",
    "            for m in range(0, int(cells_num/2)):\n",
    "                n = m + cells_num/2\n",
    "                b_celli = np.where(b_indexes == m)\n",
    "                b_cellj = np.where(b_indexes == n)\n",
    "\n",
    "                b_yij = (xprbs[b_celli] + xprbs[b_cellj])/(prbs[b_celli] + prbs[b_cellj])\n",
    "                b_yijs = np.append(b_yijs, b_yij)\n",
    "\n",
    "                b_dij = x2prbs[b_celli] + b_yij**2 * prbs[b_celli] - b_yij *2 * xprbs[b_celli]\\\n",
    "                      + x2prbs[b_cellj] + b_yij**2 * prbs[b_cellj] - b_yij *2 * xprbs[b_cellj]\n",
    "                b_dijs = np.append(b_dijs, b_dij)\n",
    "                        \n",
    "            b_distortion = np.round(sum(b_dijs), 16) \n",
    "            l3_gradient = (b_distortion - distortion)/epsilon\n",
    "            l3_gradients = np.append(l3_gradients, l3_gradient)\n",
    "            \n",
    "    l3_gradients = np.reshape(l3_gradients, [cells_num, cells_num])\n",
    "        \n",
    "#     # Now, we update the weights.   \n",
    "#     l1_weights = l1_weights + learning_rate * l1_gradients\n",
    "#     l1_biases = l1_biases + learning_rate * l1_biases_gradients\n",
    "#     l2_weights = l2_weights + learning_rate * l2_gradients    \n",
    "#     l2_biases = l2_biases + learning_rate * l2_biases_gradients\n",
    "#     l3_weights = l3_weights + learning_rate * l3_gradients\n",
    "    \n",
    "#     # We update the weights with momentum.\n",
    "#     velocity1 = past_velocity1 * momentum + learning_rate *  l1_gradients\n",
    "#     l1_weights = l1_weights - momentum * velocity1 + learning_rate * l1_gradients\n",
    "#     past_velocity1 = velocity1 \n",
    "    \n",
    "#     velocity_b1 = past_velocity_b1 * momentum + learning_rate *  l1_biases_gradients\n",
    "#     l1_biases = l1_biases - momentum * velocity_b1 + learning_rate * l1_biases_gradients\n",
    "#     past_velocity_b1 = velocity_b1\n",
    "    \n",
    "#     velocity2 = past_velocity2 * momentum + learning_rate *  l2_gradients\n",
    "#     l2_weights = l2_weights - momentum * velocity2 + learning_rate * l2_gradients\n",
    "#     past_velocity2 = velocity2\n",
    "    \n",
    "#     velocity_b2 = past_velocity_b2 * momentum + learning_rate *  l2_biases_gradients\n",
    "#     l2_biases = l2_biases - momentum * velocity_b2 + learning_rate * l2_biases_gradients\n",
    "#     past_velocity_b2 = velocity_b2\n",
    "    \n",
    "#     velocity3 = past_velocity3 * momentum + learning_rate *  l3_gradients\n",
    "#     l3_weights = l3_weights - momentum * velocity3 + learning_rate * l3_gradients\n",
    "#     past_velocity3 = velocity3\n",
    "\n",
    "\n",
    "\n",
    "    # momentum way 2\n",
    "    velocity1 = learning_rate * l1_gradients + momentum * past_velocity1\n",
    "    l1_weights = l1_weights + velocity1\n",
    "    past_velocity1 = velocity1\n",
    "    \n",
    "    velocity2 = learning_rate * l2_gradients + momentum * past_velocity2\n",
    "    l2_weights = l2_weights + velocity2\n",
    "    past_velocity2 = velocity2\n",
    "    \n",
    "    velocity_b1 = learning_rate * l1_biases_gradients + momentum * past_velocity_b1\n",
    "    l1_biases = l1_biases + velocity_b1\n",
    "    past_velocity_b1 = velocity_b1\n",
    "\n",
    "    velocity_b2 = learning_rate * l1_biases_gradients + momentum * past_velocity_b2\n",
    "    l2_biases = l2_biases + velocity_b2\n",
    "    past_velocity_b2 = velocity_b2\n",
    "\n",
    "    velocity3 = learning_rate * l3_gradients + momentum * past_velocity3\n",
    "    l3_weights = l3_weights + velocity3\n",
    "    past_velocity3 = velocity3\n",
    "\n",
    "\n",
    "# # Here, we implement the Adam algorithm.\n",
    "\n",
    "   \n",
    "#     m_adam1 = beta_1 * m_adam1 + (1 - beta_1) * l1_gradients\n",
    "#     v_adam1 = beta_2 * v_adam1 + (1 - beta_2) * np.power(l1_gradients, 2)\n",
    "#     m_hat1 = m_adam1 / (1 - np.power(beta_1, ii))\n",
    "#     v_hat1 = v_adam1 / (1 - np.power(beta_2, ii))\n",
    "#     l1_weights = l1_weights + learning_rate * m_hat1 / (np.sqrt(v_hat1) + epsilon)\n",
    "    \n",
    "#     m_adam_b1 = beta_1 * m_adam_b1 + (1 - beta_1) * l1_biases_gradients\n",
    "#     v_adam_b1 = beta_2 * v_adam_b1 + (1 - beta_2) * np.power(l1_biases_gradients, 2)\n",
    "#     m_hat_b1 = m_adam_b1 / (1 - np.power(beta_1, ii))\n",
    "#     v_hat_b1 = v_adam_b1 / (1 - np.power(beta_2, ii))\n",
    "#     l1_biases = l1_biases + learning_rate * m_hat_b1 / (np.sqrt(v_hat_b1) + epsilon)\n",
    "    \n",
    "#     m_adam2 = beta_1 * m_adam2 + (1 - beta_1) * l2_gradients\n",
    "#     v_adam2 = beta_2 * v_adam2 + (1 - beta_2) * np.power(l2_gradients, 2)\n",
    "#     m_hat2 = m_adam2 / (1 - np.power(beta_1, ii))\n",
    "#     v_hat2 = v_adam2 / (1 - np.power(beta_2, ii))\n",
    "#     l2_weights = l2_weights + learning_rate * m_hat2 / (np.sqrt(v_hat2) + epsilon)\n",
    "    \n",
    "#     m_adam_b2 = beta_1 * m_adam_b2 + (1 - beta_1) * l2_biases_gradients\n",
    "#     v_adam_b2 = beta_2 * v_adam_b2 + (1 - beta_2) * np.power(l2_biases_gradients, 2)\n",
    "#     m_hat_b2 = m_adam_b2 / (1 - np.power(beta_1, ii))\n",
    "#     v_hat_b2 = v_adam_b2 / (1 - np.power(beta_2, ii))\n",
    "#     l2_biases = l2_biases + learning_rate * m_hat_b2 / (np.sqrt(v_hat_b2) + epsilon)\n",
    "    \n",
    "#     m_adam3 = beta_1 * m_adam3 + (1 - beta_1) * l3_gradients\n",
    "#     v_adam3 = beta_2 * v_adam3 + (1 - beta_2) * np.power(l3_gradients, 2)\n",
    "#     m_hat3 = m_adam3 / (1 - np.power(beta_1, ii))\n",
    "#     v_hat3 = v_adam3 / (1 - np.power(beta_2, ii))\n",
    "#     l3_weights = l3_weights + learning_rate * m_hat3 / (np.sqrt(v_hat3) + epsilon)\n",
    "    \n",
    "        \n",
    "# Now, we calculate the outputs.\n",
    "    l1_outs = relu(np.dot(l1_weights,cell_reps)+l1_biases)\n",
    "    l2_outs = relu(np.dot(l2_weights,l1_outs)+l2_biases)\n",
    "    l3_outs = np.dot(l3_weights, l2_outs)\n",
    "    l3_outs = np.round(l3_outs, 16)\n",
    "    \n",
    "    for p in range(0, cells_num-1):\n",
    "        for q in range (p+1, cells_num):\n",
    "            if l3_outs[p] == l3_outs[q]:\n",
    "                l3_outs[q] += np.random.rand(1,1)-0.5\n",
    "   \n",
    "\n",
    "    # We assigned the indexes based on the final layer outputs.\n",
    "    sorted_outs = np.sort(l3_outs)\n",
    "    indexes = []\n",
    "    for i in range(0, cells_num):\n",
    "        index = np.where(sorted_outs == l3_outs[i])\n",
    "        indexes = np.append(indexes, index)\n",
    "\n",
    "    #print(indexes)\n",
    "    \n",
    "    yijs = [] # Yijs based on the assigned indexes\n",
    "    dijs = []\n",
    "    for i in range(0, int(cells_num/2)):\n",
    "        j = i + cells_num/2\n",
    "        celli = np.where(indexes == i)\n",
    "        cellj = np.where(indexes == j)\n",
    "\n",
    "        yij = (xprbs[celli] + xprbs[cellj])/(prbs[celli] + prbs[cellj])\n",
    "        yijs = np.append(yijs, yij)\n",
    "\n",
    "        dij = x2prbs[celli] + yij**2 * prbs[celli] - yij *2 * xprbs[celli]\\\n",
    "            + x2prbs[cellj] + yij**2 * prbs[cellj] - yij *2 * xprbs[cellj]\n",
    "        dijs = np.append(dijs, dij)\n",
    "\n",
    "    distortion = np.round(sum(dijs), 16) # The Eve's distortion based on the assigned indexes\n",
    "    print(distortion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
